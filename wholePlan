Project: Enterprise-Grade Customer Churn Prediction System

Core Technologies: Python, Poetry, Docker, Scikit-learn, Optuna, MLflow (for tracking), FastAPI, SHAP/LIME, Pandas, Numpy, Matplotlib, Seaborn.

Phase 1: Foundational Setup (Poetry, Project Structure, Tooling)

Goal: Establish a robust, reproducible, and quality-controlled development environment.

Poetry Deep Dive & Setup:

What it is: Poetry manages Python packaging and dependencies. It ensures deterministic builds by locking dependency versions (poetry.lock) and handles virtual environments automatically.

Installation: If you don't have Poetry, install it following the official instructions (usually a curl/wget script or pipx).

Project Initialization:

Open your terminal/command prompt.

Navigate to where you want your project folder to live.

Run: poetry new churn_prediction_api (This creates the basic directory structure and pyproject.toml).

cd churn_prediction_api

Understanding pyproject.toml:

[tool.poetry]: Basic project metadata (name, version, description, authors).

[tool.poetry.dependencies]: Main project dependencies (e.g., pandas, scikit-learn, fastapi). Specify Python version here (e.g., python = "^3.9").

[tool.poetry.group.dev.dependencies]: Development-only dependencies (e.g., jupyterlab, pytest, black, flake8, mypy, mlflow, optuna). Using groups (dev) keeps production environments cleaner.

[build-system]: Defines how the project is built (usually leave as default).

Adding Dependencies:

poetry add pandas numpy scikit-learn joblib python-dotenv loguru pydantic fastapi uvicorn[standard] pyarrow shap mlflow lightgbm xgboost catboost optuna (Add core runtime dependencies).

poetry add --group dev jupyterlab ipykernel pytest black flake8 isort mypy pre-commit (Add development/testing/linting tools).

Installing Dependencies:

poetry install --with dev (Installs all dependencies, including dev). In CI/CD or production Docker images, you might run poetry install --no-dev. This creates/updates poetry.lock. Commit poetry.lock to Git!

Activating Environment:

poetry shell (Spawns a new shell within the project's virtual environment).

Alternatively, run commands directly: poetry run python scripts/train_model.py or poetry run pytest.

Key Benefit: Reproducibility. Anyone cloning your repo can run poetry install and get the exact same dependency versions listed in poetry.lock.

Expert Project Structure:

Let's refine the structure for clarity and scalability:

churn_prediction_api/
├── pyproject.toml
├── poetry.lock
├── README.md
├── Dockerfile
├── docker-compose.yml  # Optional: For running services like MLflow easily
├── Makefile            # Optional: For common commands (lint, test, run)
├── .env                # For environment variables (add to .gitignore!)
├── .gitignore
├── .dockerignore
├── config/             # Configuration files (e.g., YAML, or constants)
│   └── config.yaml
│   └── logging_config.json # Or configure loguru in code
├── data/
│   └── raw/
│       └── Churn_Modelling.csv
│   └── processed/        # Output for processed data (add to .gitignore)
├── notebooks/            # EDA and experimentation
│   └── 01_eda_and_prototyping.ipynb
├── src/
│   └── churn_predictor/  # Source code package
│       ├── __init__.py
│       ├── config/       # Pydantic models for config validation
│       │   └── __init__.py
│       │   └── models.py
│       ├── processing/   # Data loading, cleaning, validation, feature engineering
│       │   └── __init__.py
│       │   └── data_manager.py
│       │   └── preprocessors.py
│       │   └── features.py
│       ├── training/     # Model training, tuning, evaluation logic
│       │   └── __init__.py
│       │   └── pipeline.py # Defines the full sklearn pipeline
│       │   └── train.py    # Orchestrates training loop, Optuna, MLflow
│       │   └── evaluate.py # Evaluation logic
│       ├── prediction/   # Prediction logic
│       │   └── __init__.py
│       │   └── predict.py
│       ├── api/          # FastAPI application
│       │   └── __init__.py
│       │   └── main.py     # FastAPI app definition
│       │   └── schemas.py  # Pydantic schemas for API requests/responses
│       │   └── endpoints/  # API route definitions
│       │       └── __init__.py
│       │       └── predict.py
│       └── core/         # Core utilities, logging setup
│           └── __init__.py
│           └── logging_setup.py
│           └── utils.py
├── tests/                # Unit and integration tests
│   ├── __init__.py
│   └── processing/
│   └── training/
│   └── api/
├── models/               # Saved model artifacts (add to .gitignore)
├── logs/                 # Log files (add to .gitignore)
└── scripts/              # High-level scripts (optional, often replaced by API/CLI)
    └── run_training.py  # Example script to trigger training pipeline


Tooling Setup:

.gitignore: Ensure standard Python ignores (__pycache__, *.pyc, .venv, *.egg-info, etc.) plus data/processed/, models/, logs/, .env, mlruns/ (for MLflow local).

.dockerignore: Similar to .gitignore, but specifically excludes files not needed in the Docker build context (e.g., .git/, notebooks/, tests/ if not running tests in Docker).

config/config.yaml (Example): Store parameters here instead of hardcoding.

data:
  raw_path: "data/raw/Churn_Modelling.csv"
  processed_train_path: "data/processed/train.csv"
  processed_test_path: "data/processed/test.csv"
  target_column: "Exited"
  test_size: 0.2
  random_state: 42

features:
  # Explicitly list features used by the pipeline
  numerical_vars: ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']
  categorical_vars: ['Geography', 'Gender'] # Example: only using these initially
  # Engineered features will be handled within the pipeline steps

training:
  models_to_train: ['LogisticRegression', 'LightGBM', 'RandomForest', 'CatBoost'] # Example
  cv_folds: 5
  optuna_trials: 100
  optimization_metric: 'roc_auc'
  mlflow_experiment_name: "Churn_Prediction_v1"
  model_output_dir: "models/"
  pipeline_save_file: "churn_pipeline.joblib"
  best_model_save_file: "best_churn_model_pipeline.joblib"

api:
  title: "Churn Prediction API"
  version: "0.1.0"
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Yaml
IGNORE_WHEN_COPYING_END

Config Loading (src/churn_predictor/config/models.py & data_manager.py): Use Pydantic for validation.

# src/churn_predictor/config/models.py
from pydantic import BaseModel, DirectoryPath, FilePath, validator
from typing import List, Optional

class DataConfig(BaseModel):
    raw_path: FilePath
    processed_train_path: str # Path handled relative to project root
    processed_test_path: str
    target_column: str
    test_size: float
    random_state: int

class FeaturesConfig(BaseModel):
    numerical_vars: List[str]
    categorical_vars: List[str]
    # Add lists for engineered features if defined statically

class TrainingConfig(BaseModel):
    models_to_train: List[str]
    cv_folds: int
    optuna_trials: int
    optimization_metric: str
    mlflow_experiment_name: str
    model_output_dir: str # Path handled relative to project root
    pipeline_save_file: str
    best_model_save_file: str

class APIConfig(BaseModel):
    title: str
    version: str

class AppConfig(BaseModel):
    data: DataConfig
    features: FeaturesConfig
    training: TrainingConfig
    api: APIConfig

# --- In data_manager.py (or a dedicated config loader) ---
import yaml
from pathlib import Path
from churn_predictor.config.models import AppConfig

CONFIG_FILE_PATH = Path(__file__).resolve().parent.parent.parent / "config/config.yaml" # Adjust path as needed
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent # Find project root

def load_config(config_path: Path = CONFIG_FILE_PATH) -> AppConfig:
    """Loads and validates config from YAML."""
    if not config_path.exists():
        raise FileNotFoundError(f"Config file not found at {config_path}")
    with open(config_path, "r") as f:
        config_dict = yaml.safe_load(f)
    return AppConfig(**config_dict)

config = load_config() # Load config globally or pass around
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Logging (src/churn_predictor/core/logging_setup.py): Use loguru for simplicity and power.

# src/churn_predictor/core/logging_setup.py
import sys
from loguru import logger
from pathlib import Path

LOGS_DIR = Path(__file__).resolve().parent.parent.parent / "logs" # Adjust path
LOGS_DIR.mkdir(exist_ok=True)

def setup_logging():
    logger.remove() # Remove default handler
    logger.add(sys.stderr, level="INFO") # Log INFO level and above to stderr
    logger.add(
        LOGS_DIR / "churn_predictor_{time}.log",
        rotation="10 MB", # Rotate log file when it reaches 10 MB
        retention="7 days", # Keep logs for 7 days
        level="DEBUG", # Log DEBUG level and above to file
        enqueue=True # Make logging asynchronous
    )
    logger.info("Logging setup complete.")

# Call setup_logging() early in your main scripts/API startup
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Pre-commit Hooks: Automate code formatting and linting.

Install pre-commit: pip install pre-commit (or use poetry run pre-commit install if poetry manages it).

Create .pre-commit-config.yaml in the root:

repos:
-   repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
    -   id: trailing-whitespace
    -   id: end-of-file-fixer
    -   id: check-yaml
    -   id: check-added-large-files
-   repo: https://github.com/psf/black
    rev: 24.3.0
    hooks:
    -   id: black
-   repo: https://github.com/pycqa/isort
    rev: 5.13.2
    hooks:
    -   id: isort
        name: isort (python)
-   repo: https://github.com/pycqa/flake8
    rev: 7.0.0
    hooks:
    -   id: flake8
# Optional: Add mypy for type checking
# -   repo: https://github.com/pre-commit/mirrors-mypy
#     rev: 'v1.9.0'
#     hooks:
#     -   id: mypy
#         additional_dependencies: [tokenize-rt==3.2.0] # Example dependency needed by mypy hook
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Yaml
IGNORE_WHEN_COPYING_END

Install the hooks: poetry run pre-commit install

Now, black, isort, flake8 will run automatically before each commit.

Phase 2: Advanced EDA & Data Validation

Goal: Deeply understand data nuances, distributions, correlations, and establish validation rules.

Tools: notebooks/01_eda_and_prototyping.ipynb, Pandas, Seaborn, Matplotlib, Scipy, ydata-profiling.

Automated Overview:

Use ydata-profiling for a quick, comprehensive report.

# In the notebook
from ydata_profiling import ProfileReport
from churn_predictor.processing.data_manager import load_raw_data # Assuming function exists
from churn_predictor.config.models import config # Load validated config

df_raw = load_raw_data(config.data.raw_path)
profile = ProfileReport(df_raw, title="Churn Data Profiling Report", explorative=True)
profile.to_file("churn_eda_report.html") # Save report
# profile.to_notebook_iframe() # Display inline in notebook
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Targeted Visualizations (Expert Level):

Distribution & Outliers: Violin plots (sns.violinplot) split by Exited, Boxen plots (sns.boxenplot) for richer quantile info.

Relationships:

sns.pairplot with hue='Exited' but only for key numerical features identified as potentially important (e.g., Age, Balance, CreditScore, NumOfProducts).

sns.jointplot with kind='kde' or kind='hist' for specific pairs vs. target.

Facet Grids (sns.FacetGrid) to explore interactions, e.g., Age distribution vs. Exited faceted by Geography.

Categorical Interactions: Heatmaps of normalized crosstabs (pd.crosstab(..., normalize=True)) or grouped bar charts.

Statistical Analysis:

Formally state hypotheses based on EDA visuals (e.g., H0: Mean age of churned = Mean age of retained).

Perform appropriate tests (scipy.stats.ttest_ind for normal-ish data, scipy.stats.mannwhitneyu for non-normal, scipy.stats.chi2_contingency for categorical). Document p-values and conclusions.

Data Validation (Optional but Recommended):

Consider libraries like pandera or great_expectations to define data schemas and validation rules (e.g., CreditScore must be between 300-850, Geography must be in ['France', 'Spain', 'Germany']). This can be integrated into the data loading step.

Phase 3: Robust Data Processing & Feature Engineering Pipeline

Goal: Create a single, serializable pipeline that handles all preprocessing and feature engineering, preventing data leakage and ensuring consistency between training and prediction.

Tools: Scikit-learn (Pipeline, ColumnTransformer, FunctionTransformer, custom transformers), Pandas, Numpy.

Data Manager (data_manager.py):

Refine load_data to handle potential errors gracefully.

Add save_data function.

Implement split_data using config values and ensuring stratification.

Add save_pipeline and load_pipeline using joblib and paths from config.

# src/churn_predictor/processing/data_manager.py
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
import joblib
from pathlib import Path
from typing import Tuple, Any
from loguru import logger

from churn_predictor.config.models import config # Import validated config
from churn_predictor.core.logging_setup import setup_logging

setup_logging() # Configure logging

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

def load_raw_data(file_path: Path = PROJECT_ROOT / config.data.raw_path) -> pd.DataFrame:
    """Loads raw data, basic validation."""
    logger.info(f"Loading raw data from {file_path}...")
    try:
        df = pd.read_csv(file_path)
        # Basic validation example
        if config.data.target_column not in df.columns:
             raise ValueError(f"Target column '{config.data.target_column}' not found in data.")
        logger.info("Raw data loaded successfully.")
        return df
    except FileNotFoundError:
        logger.error(f"Data file not found at {file_path}")
        raise
    except Exception as e:
        logger.error(f"Error loading raw data: {e}")
        raise

def drop_irrelevant_features(df: pd.DataFrame) -> pd.DataFrame:
     """Drops columns deemed irrelevant for modeling."""
     # Define irrelevant columns here or load from config
     cols_to_drop = ['RowNumber', 'CustomerId', 'Surname']
     df = df.drop(columns=cols_to_drop, errors='ignore')
     logger.info(f"Dropped irrelevant columns: {cols_to_drop}")
     return df

# --- Split Data ---
def split_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """Splits data into training and testing sets based on config."""
    logger.info("Splitting data...")
    X_train, X_test, y_train, y_test = train_test_split(
        df.drop(config.data.target_column, axis=1),
        df[config.data.target_column],
        test_size=config.data.test_size,
        random_state=config.data.random_state,
        stratify=df[config.data.target_column] # Ensure stratification
    )
    logger.info("Data split complete.")
    return X_train, X_test, y_train, y_test

# --- Pipeline Persistence ---
def save_pipeline(pipeline_to_persist: Pipeline, file_name: str = config.training.pipeline_save_file) -> None:
    """Persists the pipeline."""
    save_path = PROJECT_ROOT / config.training.model_output_dir / file_name
    save_path.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(pipeline_to_persist, save_path)
    logger.info(f"Pipeline saved to {save_path}")

def load_pipeline(file_name: str = config.training.pipeline_save_file) -> Pipeline:
    """Loads a persisted pipeline."""
    load_path = PROJECT_ROOT / config.training.model_output_dir / file_name
    if not load_path.exists():
         logger.error(f"Pipeline file not found at {load_path}")
         raise FileNotFoundError(f"Pipeline file not found at {load_path}")
    pipeline = joblib.load(load_path)
    logger.info(f"Pipeline loaded from {load_path}")
    return pipeline
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Feature Engineering within Pipeline (features.py): Create custom transformers.

# src/churn_predictor/processing/features.py
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from typing import List

class FeatureRatioCalculator(BaseEstimator, TransformerMixin):
    """Calculates specified feature ratios."""
    def __init__(self, ratio_pairs: List[tuple]):
        if not isinstance(ratio_pairs, list) or not all(isinstance(pair, tuple) and len(pair) == 2 for pair in ratio_pairs):
            raise ValueError("ratio_pairs must be a list of tuples (numerator, denominator)")
        self.ratio_pairs = ratio_pairs

    def fit(self, X, y=None):
        # No fitting needed
        return self

    def transform(self, X):
        X = X.copy()
        for num, den in self.ratio_pairs:
            # Add small epsilon to prevent division by zero
            X[f"{num}_to_{den}_Ratio"] = X[num] / (X[den] + 1e-6)
        return X

# Example: Custom transformer for combining categories or creating interaction terms
# class InteractionCreator(BaseEstimator, TransformerMixin): ...
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Preprocessing Pipeline Definition (pipeline.py): Combine everything.

# src/churn_predictor/training/pipeline.py
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer # Example: If imputation is needed

from churn_predictor.config.models import config
from churn_predictor.processing import features # Import custom feature transformers

# --- Define feature lists from config ---
NUMERICAL_VARS = config.features.numerical_vars
CATEGORICAL_VARS = config.features.categorical_vars

# --- Create Preprocessing Steps ---

# Imputation (Example: Use if missing values were found or expected)
numerical_imputer = SimpleImputer(strategy='median')
categorical_imputer = SimpleImputer(strategy='most_frequent')

# Scaling
scaler = StandardScaler()

# Encoding
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # sparse=False for easier integration

# Custom Feature Engineering
ratio_creator = features.FeatureRatioCalculator(
    ratio_pairs=[('Balance', 'EstimatedSalary'), ('CreditScore', 'Age')] # Example pairs
)
# Add other custom transformers here

# --- Define Preprocessing Pipelines for different feature types ---
numerical_pipeline = Pipeline(steps=[
    # ('imputer', numerical_imputer), # Uncomment if needed
    ('scaler', scaler)
])

categorical_pipeline = Pipeline(steps=[
    # ('imputer', categorical_imputer), # Uncomment if needed
    ('onehot', encoder)
])

# --- Use ColumnTransformer to apply pipelines to correct columns ---
# Note: Feature engineering steps might run before or after ColumnTransformer depending on design
# Option 1: Feature Engineering BEFORE branching by type
preprocessor_base = ColumnTransformer(
    transformers=[
        ('numerical', numerical_pipeline, NUMERICAL_VARS),
        ('categorical', categorical_pipeline, CATEGORICAL_VARS)
    ],
    remainder='passthrough' # Keep engineered features if created before this step
)

# Option 2: Feature Engineering AFTER branching (more complex ColumnTransformer or separate step)

# --- Combine Feature Engineering and Preprocessing ---
# This pipeline defines the sequence of transformations applied to the raw input data
# (after initial cleaning like dropping irrelevant columns)
data_processing_pipeline = Pipeline(steps=[
    ('ratio_features', ratio_creator), # Runs first on specified columns
    # Add other feature engineering steps here if needed
    ('preprocessing', preprocessor_base) # Applies scaling/encoding
    # Note: The ColumnTransformer needs updating if ratio_creator adds columns that
    # should also be scaled. Alternatively, apply scaling *after* all features are created.
    # Let's refine: It's often cleaner to have FE steps output all final features,
    # then apply a simpler ColumnTransformer just for scaling/encoding.

    # Refined Approach:
    # Step 1: Initial FE (e.g., ratios from original columns)
    # Step 2: ColumnTransformer for imputation, encoding, scaling of *all* resulting features (original + engineered)
])

# Example Refined Preprocessor (assuming ratio_creator adds 'Balance_to_EstimatedSalary_Ratio', etc.)
# Identify ALL numerical features after FE step
# ALL_NUMERICAL_VARS = NUMERICAL_VARS + ['Balance_to_EstimatedSalary_Ratio', 'CreditScore_to_Age_Ratio'] # Example
# ALL_CATEGORICAL_VARS = CATEGORICAL_VARS # Assuming no new categoricals from FE

# refined_preprocessor = ColumnTransformer(
#     transformers=[
#         ('numerical', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), ALL_NUMERICAL_VARS),
#         ('categorical', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), ALL_CATEGORICAL_VARS)
#     ],
#     remainder='drop' # Drop any columns not specified
# )

# refined_data_processing_pipeline = Pipeline(steps=[
#     ('ratio_features', ratio_creator),
#     # ('other_fe', other_feature_engineering_transformer),
#     ('preprocessing', refined_preprocessor)
# ])

# Choose the approach that best fits your feature complexity and stick to it.
# For now, let's assume the first, simpler `data_processing_pipeline`
# and adjust the `preprocessor_base` ColumnTransformer manually if FE adds new numerical columns.
# A more robust way uses `make_column_selector` from sklearn.compose.
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Phase 4: Advanced Model Training & Experiment Tracking

Goal: Train multiple models efficiently, tune them rigorously using Optuna, and log all experiments using MLflow for comparison and reproducibility.

Tools: train.py, Scikit-learn models (LogisticRegression, RandomForestClassifier), LightGBM, XGBoost, CatBoost, Optuna, MLflow.

MLflow Setup:

Tracking Server (Optional but recommended for teams): Can be set up locally (e.g., mlflow server --host 127.0.0.1 --port 8080) or use a hosted service like Databricks. For local, logs go to mlruns/ by default.

Integration: Use mlflow.sklearn.autolog() for automatic logging or manually log parameters, metrics, and artifacts. Manual logging gives more control.

Refined Training Script (train.py):

# src/churn_predictor/training/train.py
import optuna
import mlflow
import mlflow.sklearn
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_auc_score, f1_score, get_scorer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.pipeline import Pipeline
from loguru import logger

from churn_predictor.config.models import config
from churn_predictor.processing.data_manager import split_data, save_pipeline, load_raw_data, drop_irrelevant_features
from churn_predictor.training.pipeline import data_processing_pipeline # Import the defined processing pipeline
from churn_predictor.core.logging_setup import setup_logging

setup_logging()

# --- Model Definitions ---
MODELS = {
    "LogisticRegression": LogisticRegression(random_state=config.data.random_state, max_iter=1000, class_weight='balanced'),
    "RandomForest": RandomForestClassifier(random_state=config.data.random_state, class_weight='balanced', n_jobs=-1),
    "LightGBM": LGBMClassifier(random_state=config.data.random_state, class_weight='balanced', n_jobs=-1),
    "XGBoost": XGBClassifier(random_state=config.data.random_state, scale_pos_weight=(1 / config.data.test_size) - 1, use_label_encoder=False, eval_metric='logloss', n_jobs=-1), # Handle imbalance
    "CatBoost": CatBoostClassifier(random_state=config.data.random_state, auto_class_weights='Balanced', verbose=0) # Built-in imbalance handling
}

# --- Optuna Hyperparameter Spaces ---
def get_optuna_params(trial, model_name):
    if model_name == "LogisticRegression":
        return {
            "classifier__C": trial.suggest_float("C", 1e-4, 1e2, log=True),
            "classifier__solver": trial.suggest_categorical("solver", ["liblinear", "saga"]),
             # Dynamically adjust penalty based on solver in objective if needed
            "classifier__penalty": trial.suggest_categorical("penalty", ["l1", "l2"])
        }
    elif model_name == "RandomForest":
        return {
            "classifier__n_estimators": trial.suggest_int("n_estimators", 50, 600),
            "classifier__max_depth": trial.suggest_int("max_depth", 3, 30),
            "classifier__min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
            "classifier__min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 20)
            # class_weight already set to balanced
        }
    elif model_name == "LightGBM":
         return {
            'classifier__n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'classifier__learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'classifier__num_leaves': trial.suggest_int('num_leaves', 20, 300),
            'classifier__max_depth': trial.suggest_int('max_depth', 3, 15),
            'classifier__reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'classifier__reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'classifier__colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),
            'classifier__subsample': trial.suggest_float('subsample', 0.4, 1.0),
         }
    elif model_name == "XGBoost":
         return {
            'classifier__n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'classifier__learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'classifier__max_depth': trial.suggest_int('max_depth', 3, 15),
            'classifier__subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'classifier__colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
            'classifier__gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),
            'classifier__reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'classifier__reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
         }
    elif model_name == "CatBoost":
        return {
            'classifier__iterations': trial.suggest_int('iterations', 100, 1000),
            'classifier__learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'classifier__depth': trial.suggest_int('depth', 4, 10),
            'classifier__l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),
            'classifier__border_count': trial.suggest_int('border_count', 32, 255),
        }
    # Add SVC or others if needed
    else:
        return {}

# --- Optuna Objective Function ---
def objective(trial, model_name, X_train, y_train):
    base_model = MODELS[model_name]
    pipeline = Pipeline(steps=[
        ('processing', data_processing_pipeline), # Full data processing pipeline
        ('classifier', base_model)
    ])

    # Get hyperparameters to tune for this trial
    params = get_optuna_params(trial, model_name)

    # Handle specific incompatible params (e.g., penalty/solver for LogisticRegression)
    if model_name == "LogisticRegression":
        solver = params.get("classifier__solver", "liblinear")
        penalty = params.get("classifier__penalty", "l2")
        if solver == "saga" and penalty == "l1": # saga supports l1/l2/elasticnet/none
            pass
        elif solver == "liblinear" and penalty not in ["l1", "l2"]:
             logger.warning(f"Invalid penalty '{penalty}' for liblinear. Using 'l2'.")
             params["classifier__penalty"] = "l2"
        elif solver != "liblinear" and solver != "saga" and penalty == "l1":
             # Other solvers might not support l1
             logger.warning(f"Solver '{solver}' may not support l1 penalty. Trying 'l2'.")
             params["classifier__penalty"] = "l2"

    pipeline.set_params(**params)

    # Use MLflow within the objective for tracking each trial
    with mlflow.start_run(nested=True): # Nested run for each Optuna trial
        mlflow.log_params(trial.params) # Log Optuna suggested params
        mlflow.log_param("model_name", model_name)

        try:
            score = cross_val_score(
                pipeline, X_train, y_train,
                n_jobs=-1, cv=config.training.cv_folds,
                scoring=config.training.optimization_metric
            ).mean()
            mlflow.log_metric(f"cv_{config.training.optimization_metric}", score)
        except Exception as e:
             logger.error(f"Trial failed for {model_name} with params {trial.params}: {e}")
             mlflow.log_metric(f"cv_{config.training.optimization_metric}", -1) # Log failure
             # Optional: Prune trial if specific errors occur
             # raise optuna.exceptions.TrialPruned()
             return -1 # Indicate failure to Optuna

    return score

# --- Main Training Orchestration ---
def run_training():
    """Loads data, runs training pipeline with tuning and MLflow logging."""
    df_raw = load_raw_data()
    df_clean = drop_irrelevant_features(df_raw)
    X_train, X_test, y_train, y_test = split_data(df_clean)

    mlflow.set_experiment(config.training.mlflow_experiment_name)

    best_overall_pipeline = None
    best_overall_score = -1
    best_model_name = None

    for model_name in config.training.models_to_train:
        logger.info(f"\n===== Tuning Model: {model_name} =====")
        with mlflow.start_run(run_name=f"Tune_{model_name}"): # Parent run for each model type tuning process
            mlflow.log_param("model_type", model_name)

            study = optuna.create_study(direction="maximize")
            try:
                 study.optimize(
                    lambda trial: objective(trial, model_name, X_train, y_train),
                    n_trials=config.training.optuna_trials,
                    n_jobs=-1 # Use multiple cores if possible
                )
            except Exception as e:
                logger.error(f"Optuna optimization failed for {model_name}: {e}")
                continue # Skip to next model

            # Log best params and score from Optuna to the parent MLflow run
            mlflow.log_params(study.best_params)
            mlflow.log_metric(f"best_cv_{config.training.optimization_metric}", study.best_value)

            logger.info(f"Best CV {config.training.optimization_metric} for {model_name}: {study.best_value:.4f}")
            logger.info(f"Best Params: {study.best_params}")

            # Train final model with best params on full training set
            final_model = MODELS[model_name]
            final_pipeline = Pipeline(steps=[
                ('processing', data_processing_pipeline),
                ('classifier', final_model)
            ])
            # Set the best parameters found by Optuna
            final_pipeline.set_params(**study.best_params)

            logger.info(f"Fitting final {model_name} model...")
            final_pipeline.fit(X_train, y_train)
            logger.info("Final model fitting complete.")

            # Evaluate on test set and log metrics
            test_roc_auc = roc_auc_score(y_test, final_pipeline.predict_proba(X_test)[:, 1])
            test_f1 = f1_score(y_test, final_pipeline.predict(X_test))
            mlflow.log_metric("test_roc_auc", test_roc_auc)
            mlflow.log_metric("test_f1_score", test_f1)
            logger.info(f"Test ROC AUC for {model_name}: {test_roc_auc:.4f}")
            logger.info(f"Test F1 Score for {model_name}: {test_f1:.4f}")

            # Log the final fitted pipeline as an artifact
            mlflow.sklearn.log_model(final_pipeline, f"{model_name}_pipeline")

            # Save the pipeline locally as well
            save_pipeline(final_pipeline, file_name=f"{model_name}_pipeline.joblib")

            # Update best overall model if current is better
            if study.best_value > best_overall_score:
                best_overall_score = study.best_value
                best_model_name = model_name
                best_overall_pipeline = final_pipeline


    if best_overall_pipeline:
        logger.info(f"\nBest performing model type based on CV {config.training.optimization_metric}: {best_model_name} ({best_overall_score:.4f})")
        # Save the best overall pipeline with a generic name
        save_pipeline(best_overall_pipeline, file_name=config.training.best_model_save_file)
        # Optionally log the *best* pipeline again with a specific tag/alias in MLflow
        with mlflow.start_run(run_name="Best_Model_Overall"):
             mlflow.sklearn.log_model(best_overall_pipeline, "best_model")
             mlflow.log_param("best_model_type", best_model_name)
             # Re-evaluate on test set for clarity
             test_roc_auc = roc_auc_score(y_test, best_overall_pipeline.predict_proba(X_test)[:, 1])
             test_f1 = f1_score(y_test, best_overall_pipeline.predict(X_test))
             mlflow.log_metric("final_test_roc_auc", test_roc_auc)
             mlflow.log_metric("final_test_f1_score", test_f1)
    else:
        logger.warning("No models were successfully trained and tuned.")


if __name__ == "__main__":
    run_training()
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Phase 5: Insightful Evaluation & Explainability (SHAP/LIME)

Goal: Evaluate the best model thoroughly and understand why it makes certain predictions.

Tools: evaluate.py (enhanced), Matplotlib, Seaborn, SHAP, LIME.

Enhanced Evaluation (evaluate.py): Add more metrics (Balanced Accuracy, MCC) and plotting functions for ROC and Precision-Recall curves.

SHAP Implementation Guide (Run in notebook or separate script after training):

Install: Already added via poetry add shap.

Choose Explainer:

shap.TreeExplainer: Optimized for tree-based models (RandomForest, LightGBM, XGBoost, CatBoost). Faster and more accurate for these.

shap.KernelExplainer: Model-agnostic, works for any model (including pipelines with complex preprocessing, SVMs, Logistic Regression) but can be slower. Requires a background dataset.

shap.LinearExplainer: For linear models.

Steps (using TreeExplainer for a tree model like LightGBM/RF/XGB/CatBoost):

import shap
import matplotlib.pyplot as plt
from churn_predictor.processing.data_manager import load_pipeline, load_raw_data, drop_irrelevant_features, split_data
from churn_predictor.config.models import config
import pandas as pd

# Load the best trained pipeline (assuming it's tree-based)
# Use the specific model name or the 'best_model_pipeline.joblib'
pipeline = load_pipeline(file_name=config.training.best_model_save_file) # Or e.g., "LightGBM_pipeline.joblib"

# Load and split data again to get the test set features *before* preprocessing
df_raw = load_raw_data()
df_clean = drop_irrelevant_features(df_raw)
X_train, X_test, y_train, y_test = split_data(df_clean) # Use the same split

# Get the preprocessing and model steps
preprocessor_step = pipeline.named_steps['processing'] # Adjust name if different
model_step = pipeline.named_steps['classifier'] # Adjust name if different

# Transform the test data using the fitted preprocessor
# Need to handle potential new columns from OneHotEncoder correctly
# It's easier if the preprocessor outputs a DataFrame with meaningful column names
# If using sklearn >= 1.0, ColumnTransformer + OneHotEncoder can output feature names
try:
     # Requires sklearn >= 1.0 and sparse_output=False in OneHotEncoder
     feature_names_out = preprocessor_step.get_feature_names_out()
     X_test_processed = pd.DataFrame(
         preprocessor_step.transform(X_test),
         columns=feature_names_out,
         index=X_test.index
     )
except AttributeError:
     logger.warning("Could not get feature names from preprocessor. SHAP plots might have generic names.")
     X_test_processed = preprocessor_step.transform(X_test)
     # If it's numpy, try to reconstruct a DataFrame if possible, otherwise use numpy array

# 1. Create Explainer (for tree models)
explainer = shap.TreeExplainer(model_step, data=X_test_processed) # Pass processed data if model depends on feature names/stats

# 2. Calculate SHAP values (this can take time)
logger.info("Calculating SHAP values...")
shap_values = explainer.shap_values(X_test_processed)
logger.info("SHAP values calculated.")

# Note: For binary classification, shap_values might be a list [shap_values_class_0, shap_values_class_1]
# or just the values for the positive class depending on the model/explainer.
# Adjust indexing if needed, usually we focus on the positive class (Churn=1)
shap_values_positive_class = shap_values[1] if isinstance(shap_values, list) and len(shap_values)>1 else shap_values

# 3. Global Explanation: Summary Plot (Feature Importance)
logger.info("Generating SHAP summary plot...")
shap.summary_plot(shap_values_positive_class, X_test_processed, plot_type="bar", show=False)
plt.title("SHAP Global Feature Importance (Impact on Churn Probability)")
plt.tight_layout()
plt.savefig("shap_summary_bar.png") # Save the plot
plt.close()

shap.summary_plot(shap_values_positive_class, X_test_processed, show=False)
plt.title("SHAP Summary Plot (Value Distribution vs. Impact)")
plt.tight_layout()
plt.savefig("shap_summary_dot.png")
plt.close()
logger.info("SHAP summary plots saved.")

# 4. Dependence Plots (Interaction Effects) - select important features
important_features = X_test_processed.columns # Or select top N based on summary plot
for feature in important_features[:5]: # Example: Top 5
    try:
        logger.info(f"Generating SHAP dependence plot for {feature}...")
        shap.dependence_plot(feature, shap_values_positive_class, X_test_processed, interaction_index="auto", show=False)
        plt.title(f"SHAP Dependence Plot: {feature}")
        plt.tight_layout()
        plt.savefig(f"shap_dependence_{feature}.png")
        plt.close()
    except Exception as e:
        logger.warning(f"Could not generate dependence plot for {feature}: {e}")
logger.info("SHAP dependence plots generated.")

# 5. Local Explanation: Force Plot (Single Prediction)
# Explain the prediction for the first test sample
logger.info("Generating SHAP force plot for a single instance...")
# Need the explainer's expected value (base rate)
expected_value = explainer.expected_value[1] if isinstance(explainer.expected_value, (list, np.ndarray)) else explainer.expected_value

shap.force_plot(expected_value,
                shap_values_positive_class[0,:], # SHAP values for first instance
                X_test_processed.iloc[0,:], # Feature values for first instance
                matplotlib=True, # Use matplotlib for saving
                show=False)
plt.title("SHAP Force Plot (Instance 0)")
plt.savefig("shap_force_plot_instance_0.png", bbox_inches='tight')
plt.close()
logger.info("SHAP force plot saved.")

# Note: For KernelExplainer, initialization is different:
# background_data = shap.kmeans(X_train_processed, 50) # Sample background data
# explainer = shap.KernelExplainer(pipeline.predict_proba, background_data)
# shap_values = explainer.shap_values(X_test_processed) # Usually gives [class0, class1]
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

LIME Implementation Guide (Alternative/Complementary to SHAP):

Install: poetry add lime

Steps (in notebook or script):

import lime
import lime.lime_tabular
import numpy as np

# Assume pipeline, X_train, X_test, y_train, y_test, preprocessor_step are loaded/defined
# Assume X_train_processed, X_test_processed are DataFrames with correct feature names

# 1. Create LimeTabularExplainer
# Needs training data (after processing), feature names, class names, and mode (classification/regression)
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train_processed), # LIME often prefers numpy arrays
    feature_names=X_train_processed.columns.tolist(),
    class_names=['Not Exited', 'Exited'],
    mode='classification'
)

# 2. Explain a specific instance
instance_index = 5 # Example: Explain the 6th instance in the test set
instance_to_explain = X_test_processed.iloc[instance_index]
true_label = y_test.iloc[instance_index]

logger.info(f"Explaining instance {instance_index} (True Label: {true_label})...")

# LIME needs a function that takes data (numpy array) and returns prediction probabilities
def predict_proba_lime(data):
    # Need to handle potential difference in feature order/format if LIME perturbs
    # Safest is to reconstruct DataFrame if explainer was trained on one
    try:
        data_df = pd.DataFrame(data, columns=X_train_processed.columns)
        # Apply ONLY the classifier step, as input `data` is assumed to be preprocessed
        probas = pipeline.named_steps['classifier'].predict_proba(data_df)
        return probas
    except Exception as e:
         logger.error(f"Error in LIME predict_proba wrapper: {e}")
         # Return dummy probabilities on error
         return np.array([[0.5, 0.5]] * len(data))


explanation = explainer.explain_instance(
    data_row=np.array(instance_to_explain),
    predict_fn=predict_proba_lime,
    num_features=10 # Show top 10 features influencing the prediction
)

# 3. Visualize/Save Explanation
logger.info(f"LIME Explanation for instance {instance_index}:")
# Print explanation to console
for feature, weight in explanation.as_list():
    print(f"{feature}: {weight:.4f}")

# Save explanation to HTML file
explanation.save_to_file(f'lime_explanation_instance_{instance_index}.html')
logger.info(f"LIME explanation saved to HTML file.")
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Phase 6: API Deployment with FastAPI

Goal: Create a simple, robust API endpoint to serve predictions using the best-trained pipeline.

Tools: FastAPI, Uvicorn, Pydantic, api/ directory structure.

Define API Schemas (api/schemas.py):

# src/churn_predictor/api/schemas.py
from pydantic import BaseModel, Field
from typing import Optional

# Define input schema - matching columns *before* preprocessing
# Add validation using Field if needed (e.g., ge/le for numerical)
class ChurnPredictionInput(BaseModel):
    CreditScore: int
    Geography: str # Consider Enum ['France', 'Spain', 'Germany']
    Gender: str    # Consider Enum ['Male', 'Female']
    Age: int
    Tenure: int
    Balance: float
    NumOfProducts: int
    HasCrCard: int # Assuming 0 or 1
    IsActiveMember: int # Assuming 0 or 1
    EstimatedSalary: float

    class Config:
        schema_extra = {
            "example": {
                "CreditScore": 608, "Geography": "Spain", "Gender": "Female",
                "Age": 41, "Tenure": 1, "Balance": 83807.86, "NumOfProducts": 1,
                "HasCrCard": 0, "IsActiveMember": 1, "EstimatedSalary": 112542.58
            }
        }

class ChurnPredictionOutput(BaseModel):
    prediction: int # 0 or 1
    probability_churn: Optional[float] = None # Probability of class 1 (Churn)
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Create Prediction Endpoint (api/endpoints/predict.py):

# src/churn_predictor/api/endpoints/predict.py
from fastapi import APIRouter, HTTPException
from loguru import logger
import pandas as pd

from churn_predictor.prediction import predict # Import prediction logic function
from churn_predictor.api.schemas import ChurnPredictionInput, ChurnPredictionOutput
from churn_predictor.config.models import config # Load config if needed for labels etc.

router = APIRouter()

@router.post("/predict", response_model=ChurnPredictionOutput, name="predict")
async def post_predict(input_data: ChurnPredictionInput) -> ChurnPredictionOutput:
    """Make a churn prediction based on input customer data."""
    try:
        # Convert Pydantic model to DataFrame (required by pipeline)
        input_df = pd.DataFrame([input_data.dict()])
        logger.info(f"Received prediction request: {input_data.dict()}")

        # Call the prediction logic (which loads the pipeline and predicts)
        results = predict.make_prediction(input_data=input_df)

        prediction = results["predictions"][0] # Assuming make_prediction returns list
        probability = results["probabilities"][0] if results.get("probabilities") else None

        logger.info(f"Prediction results: Prediction={prediction}, Probability={probability}")
        return ChurnPredictionOutput(prediction=prediction, probability_churn=probability)

    except FileNotFoundError as e:
         logger.error(f"Prediction error: Model pipeline not found. {e}")
         raise HTTPException(status_code=500, detail="Model not ready")
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        # Be careful not to expose sensitive error details to the client
        raise HTTPException(status_code=500, detail=f"Internal server error during prediction: {e}")
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Prediction Logic (prediction/predict.py):

# src/churn_predictor/prediction/predict.py
import pandas as pd
from loguru import logger
from typing import Dict, List, Optional, Union
import numpy as np

from churn_predictor.processing.data_manager import load_pipeline
from churn_predictor.config.models import config

# Load pipeline once when the module is imported or via dependency injection
try:
    _churn_pipeline = load_pipeline(file_name=config.training.best_model_save_file)
    logger.info("Prediction pipeline loaded successfully.")
except Exception as e:
    _churn_pipeline = None
    logger.error(f"Failed to load prediction pipeline on startup: {e}")


def make_prediction(*, input_data: pd.DataFrame) -> Dict[str, Optional[Union[List[int], List[float]]]]:
    """Make predictions using the saved pipeline."""
    if _churn_pipeline is None:
         logger.error("Prediction pipeline is not loaded.")
         raise RuntimeError("Prediction model is not available.")

    # Ensure input is DataFrame
    if not isinstance(input_data, pd.DataFrame):
        input_data = pd.DataFrame(input_data)

    # Basic validation on input_data columns could be added here

    logger.info(f"Making prediction on {len(input_data)} samples...")
    predictions = _churn_pipeline.predict(input_data)

    results: Dict[str, Optional[Union[List[int], List[float]]]] = {"predictions": predictions.tolist()} # Ensure serializable type

    # Get probabilities if the model supports it
    if hasattr(_churn_pipeline, "predict_proba"):
         probabilities = _churn_pipeline.predict_proba(input_data)[:, 1] # Probability of positive class (Churn=1)
         results["probabilities"] = probabilities.tolist()
    else:
         results["probabilities"] = None

    logger.info("Prediction completed.")
    return results
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

FastAPI App (api/main.py):

# src/churn_predictor/api/main.py
from fastapi import FastAPI, APIRouter
from loguru import logger

from churn_predictor.api.endpoints import predict as predict_endpoint # Relative import
from churn_predictor.config.models import config
from churn_predictor.core.logging_setup import setup_logging

setup_logging() # Setup logging on startup

app = FastAPI(
    title=config.api.title,
    version=config.api.version,
    description="API to predict customer churn."
)

# Include routers from endpoint modules
api_router = APIRouter()
api_router.include_router(predict_endpoint.router, tags=["Prediction"])

app.include_router(api_router)

@app.get("/", tags=["Health"])
async def health_check():
    logger.info("Health check endpoint called.")
    return {"status": "ok"}

logger.info("FastAPI application started.")

# To run: uvicorn churn_predictor.api.main:app --reload (for development)
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Update Dockerfile for API:

# ... (Previous stages for installing Python, Poetry, dependencies) ...

WORKDIR /app
COPY . .

# Expose the port FastAPI will run on
EXPOSE 8000

# Command to run the FastAPI application using Uvicorn
CMD ["uvicorn", "src.churn_predictor.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Dockerfile
IGNORE_WHEN_COPYING_END

Build: docker build -t churn-predictor-api .

Run: docker run -p 8000:8000 churn-predictor-api

Phase 7: Final GitHub Repository & README

Goal: Create a professional, well-documented repository showcasing the project.

Final Code Review: Ensure all code is clean, commented, type-hinted, and follows PEP 8 (using black and flake8).

README Enhancement:

Add Visualizations: Embed key EDA plots (e.g., churn rate, important feature distributions vs. churn) and SHAP summary plots directly in the README (use relative paths if storing images in the repo, e.g., in an images/ folder).

Results Table: Create a Markdown table summarizing the performance (ROC AUC, F1, Precision, Recall, etc.) of all tuned models on the test set. Highlight the best model.

Explainability Insights: Briefly summarize the key drivers of churn identified by SHAP/LIME.

API Usage: Provide clear curl examples for hitting the /predict endpoint.

Docker Instructions: Make building and running the API container explicit.

MLflow: Explain how to view experiment results (e.g., run mlflow ui in the project directory).

GitHub: Push all code, README.md, LICENSE, configuration files, .gitignore, .dockerignore, Dockerfile, pyproject.toml, poetry.lock, .pre-commit-config.yaml to your repository. Ensure sensitive files (.env, data/processed, models, logs) are not committed. Use appropriate tags.

This comprehensive, expert-level plan sets you up for a truly outstanding portfolio piece. It emphasizes structure, reproducibility, automation, advanced techniques, and production readiness, moving far beyond typical tutorial projects. Remember to execute each step carefully and document your reasoning, especially for design choices and EDA findings.