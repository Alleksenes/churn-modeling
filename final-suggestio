Okay, understood. Apologies for the overwhelming suggestions. Let's consolidate and apply the crucial fixes directly into the code files you provided.

The goal is to make these files as robust and runnable as possible based on our discussion, addressing potential errors and incorporating best practices.

Here are the complete, revised files based on your structure and the previous corrections:

```python
# ============================================
# File: src/churn_model/__init__.py
# ============================================
# This file makes src/churn_model a Python package
from pathlib import Path

# Define project root robustly, assuming this file is at src/churn_model/__init__.py
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

# Make key modules easily accessible if desired (optional)
# from . import config
# from . import utils
# from . import processing
# from . import pipeline
# from . import tune
# from . import train
# from . import evaluate
# from . import predict

# NOTE: Explicitly importing submodules here can sometimes lead to circular dependencies
# if they import from each other at the top level. It's often safer to let other
# modules import directly, e.g., `from src.churn_model.config import load_config`.
```

```python
# ============================================
# File: src/churn_model/config.py
# ============================================
import yaml
from pydantic import BaseModel, FilePath, Field, validator, DirectoryPath
from typing import List, Optional, Dict, Any
from pathlib import Path
from loguru import logger

# Define PROJECT_ROOT relative to this file's location
# This assumes config.py is always at src/churn_model/config.py
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
CONFIG_FILE_PATH = PROJECT_ROOT / "config.yaml"

# --- Pydantic Models for Validation ---

class DataConfig(BaseModel):
    raw_path: str # Relative to project root
    processed_train_path: str
    processed_test_path: str
    target_column: str
    test_size: float = Field(..., gt=0, lt=1)
    random_state: int
    initial_features: List[str]
    numerical_vars: List[str]
    categorical_vars: List[str]

    @validator('raw_path', 'processed_train_path', 'processed_test_path', pre=True)
    def make_path_absolute(cls, v):
        path = Path(v)
        if path.is_absolute():
            return str(path) # Already absolute
        return str(PROJECT_ROOT / v) # Make absolute relative to project root

    # Use root_validator for checks involving multiple fields after individual validation
    @validator('categorical_vars')
    def check_features_overlap(cls, cat_vars, values):
        # This validator runs *after* numerical_vars is validated and available in values
        if 'numerical_vars' in values and values['numerical_vars']:
            num_set = set(values['numerical_vars'])
            cat_set = set(cat_vars)
            overlap = num_set.intersection(cat_set)
            if overlap:
                raise ValueError(f"Features cannot be both numerical and categorical: {overlap}")
        return cat_vars

    @validator('numerical_vars', 'categorical_vars', each_item=False) # Check the whole list
    def check_features_in_initial(cls, vars_list, values):
         # This validator runs *after* initial_features is validated and available in values
         if 'initial_features' in values and values['initial_features']:
             initial_set = set(values['initial_features'])
             if not set(vars_list).issubset(initial_set):
                  missing = set(vars_list) - initial_set
                  raise ValueError(f"Features {missing} not in initial_features list in config")
         return vars_list


class TuningConfig(BaseModel):
    models_to_tune: List[str]
    cv_folds: int = Field(..., gt=1)
    optuna_trials_per_model: int = Field(..., gt=0)
    optimization_metric: str
    mlflow_experiment_name: str
    best_params_output_path: str # Relative to project root

    @validator('best_params_output_path', pre=True)
    def make_path_absolute(cls, v):
        path = Path(v)
        if path.is_absolute(): return str(path)
        return str(PROJECT_ROOT / v)

class TrainingConfig(BaseModel):
    best_params_input_path: str # Relative to project root
    final_model_output_path: str # Relative to project root
    mlflow_experiment_name: str

    @validator('best_params_input_path', 'final_model_output_path', pre=True)
    def make_path_absolute(cls, v):
        path = Path(v)
        if path.is_absolute(): return str(path)
        return str(PROJECT_ROOT / v)

class EvaluationConfig(BaseModel):
    model_input_path: str # Relative to project root
    plots_output_dir: str # Relative to project root
    shap_plots_output_dir: str # Relative to project root
    mlflow_experiment_name: str
    shap_kernel_background_samples: int = Field(..., gt=0)

    @validator('model_input_path', 'plots_output_dir', 'shap_plots_output_dir', pre=True)
    def make_path_absolute(cls, v):
        path = Path(v)
        if path.is_absolute(): return str(path)
        return str(PROJECT_ROOT / v)


class APIConfig(BaseModel):
    title: str
    version: str

class AppConfig(BaseModel):
    project_name: str
    version: str
    data: DataConfig
    tuning: TuningConfig
    training: TrainingConfig
    evaluation: EvaluationConfig
    api: APIConfig

# --- Loading Function ---

def load_config(config_path: Path = CONFIG_FILE_PATH) -> AppConfig:
    """Loads and validates config from YAML."""
    if not config_path.exists():
        logger.critical(f"CRITICAL: Config file not found at {config_path}") # Use critical for fatal errors
        raise FileNotFoundError(f"Config file not found at {config_path}")
    try:
        with open(config_path, "r") as f:
            config_dict = yaml.safe_load(f)
        # Validate the entire structure
        validated_config = AppConfig(**config_dict)
        logger.info(f"Configuration loaded and validated from {config_path}")
        return validated_config
    except yaml.YAMLError as e:
        logger.critical(f"CRITICAL: Error parsing YAML config file {config_path}: {e}")
        raise
    except Exception as e: # Catch Pydantic validation errors too
        logger.critical(f"CRITICAL: Error loading or validating config file {config_path}: {e}", exc_info=True)
        raise
```

```python
# ============================================
# File: src/churn_model/utils.py
# ============================================
import sys
from loguru import logger
from pathlib import Path
import json
import joblib
from typing import Any, Dict # Corrected import

# Import PROJECT_ROOT from config.py where it's robustly defined
from .config import PROJECT_ROOT

LOGS_DIR = PROJECT_ROOT / "logs"
CONFIG_DIR = PROJECT_ROOT / "config" # Directory containing logging_config.json (if used)

DEFAULT_LOGGING_CONFIG = {
    "handlers": [
        {"sink": sys.stderr, "level": "INFO", "format": "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"},
        {
            "sink": LOGS_DIR / "churn_model_{time}.log",
            "rotation": "10 MB",
            "retention": "10 days",
            "level": "DEBUG",
            "enqueue": True,
            "backtrace": True,
            "diagnose": True,
            "format": "{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}"
        },
    ]
}

_logging_configured = False

def setup_logging(config_path: Path = CONFIG_DIR / "logging_config.json"):
    """Configures Loguru logger. Ensures it only runs once per process."""
    global _logging_configured
    if _logging_configured:
        logger.trace("Logging already configured.")
        return

    try:
        LOGS_DIR.mkdir(parents=True, exist_ok=True) # Ensure logs dir exists
    except OSError as e:
         # Use basic print/stderr if logger setup itself fails critically
         print(f"ERROR: Could not create logs directory {LOGS_DIR}: {e}", file=sys.stderr)
         # Attempt to configure stderr logging at least
         try:
             logger.remove()
             logger.add(sys.stderr, level="INFO")
             logger.error("Failed to create log directory, using stderr only.")
             _logging_configured = True
         except Exception:
             print("CRITICAL: Failed even to configure stderr logging.", file=sys.stderr)
         return # Stop further configuration attempts

    logger.remove() # Remove default handler before configuring
    config_data = DEFAULT_LOGGING_CONFIG
    if config_path.exists():
        try:
            with open(config_path, 'r') as f:
                config_data = json.load(f)
            # Use logger here only if basic stderr is already added or confident it works
            # logger.info(f"Loaded logging configuration from {config_path}")
            print(f"INFO: Loaded logging configuration from {config_path}") # Safer print during setup
        except Exception as e:
            print(f"WARNING: Failed to load logging config from {config_path}, using defaults. Error: {e}", file=sys.stderr)
            config_data = DEFAULT_LOGGING_CONFIG
    else:
        print(f"INFO: Logging config file not found at {config_path}, using default configuration.")

    try:
        logger.configure(**config_data)
        logger.info("Logging setup complete.") # Now logger is configured
        _logging_configured = True
    except Exception as e:
        # Fallback to basic stderr logging if configuration fails
        logger.add(sys.stderr, level="INFO")
        logger.error(f"Failed to configure logging with provided settings: {e}. Using basic stderr logging.")
        _logging_configured = True # Mark as configured to prevent loops

# --- Generic Save/Load Helpers ---

def save_json(data: Dict, file_path: Path):
    """Saves dictionary data to JSON file."""
    try:
        file_path.parent.mkdir(parents=True, exist_ok=True)
        with open(file_path, 'w') as f:
            json.dump(data, f, indent=4)
        logger.info(f"Successfully saved JSON to {file_path}")
    except TypeError as e:
        logger.error(f"Data type error saving JSON to {file_path}: {e}. Data: {str(data)[:200]}...") # Log snippet
        raise
    except Exception as e:
        logger.error(f"Error saving JSON to {file_path}: {e}", exc_info=True)
        raise

def load_json(file_path: Path) -> Dict:
    """Loads dictionary data from JSON file."""
    if not file_path.exists():
        logger.error(f"JSON file not found at {file_path}")
        raise FileNotFoundError(f"JSON file not found at {file_path}")
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        logger.info(f"Successfully loaded JSON from {file_path}")
        return data
    except json.JSONDecodeError as e:
         logger.error(f"Error decoding JSON from {file_path}: {e}")
         raise
    except Exception as e:
        logger.error(f"Error loading JSON from {file_path}: {e}", exc_info=True)
        raise

def save_pipeline_joblib(pipeline: Any, file_path: Path):
    """Saves any object (like a pipeline) using joblib."""
    try:
        file_path.parent.mkdir(parents=True, exist_ok=True)
        joblib.dump(pipeline, file_path)
        logger.info(f"Object saved successfully using joblib to {file_path}")
    except Exception as e:
        logger.error(f"Error saving object with joblib to {file_path}: {e}", exc_info=True)
        raise

def load_pipeline_joblib(file_path: Path) -> Any:
    """Loads an object (like a pipeline) using joblib."""
    if not file_path.exists():
         logger.error(f"Joblib file not found at {file_path}")
         raise FileNotFoundError(f"Joblib file not found at {file_path}")
    try:
        pipeline = joblib.load(file_path)
        logger.info(f"Object loaded successfully using joblib from {file_path}")
        return pipeline
    except Exception as e:
        # More specific error logging can be helpful
        logger.error(f"Error loading object with joblib from {file_path}: {e}", exc_info=True)
        raise
```

```python
# ============================================
# File: src/churn_model/processing.py
# ============================================
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np
from pathlib import Path
from typing import Tuple, List, Dict, Any
from loguru import logger
import argparse # Added for optional command-line execution

# Import config types and PROJECT_ROOT
from .config import AppConfig, DataConfig, PROJECT_ROOT
from .utils import setup_logging # Import setup_logging

# --- Setup Logging ---
# setup_logging() # Call explicitly in main scripts or __main__ block

# --- Custom Transformers ---

class FeatureRatioCalculator(BaseEstimator, TransformerMixin):
    """Calculates specified feature ratios."""
    def __init__(self, ratio_pairs: List[Tuple[str, str]], epsilon: float = 1e-6):
        if not isinstance(ratio_pairs, list) or not all(isinstance(pair, tuple) and len(pair) == 2 for pair in ratio_pairs):
            raise ValueError("ratio_pairs must be a list of tuples (numerator, denominator)")
        self.ratio_pairs = ratio_pairs
        self.epsilon = epsilon
        self._feature_names_out = []

    def fit(self, X: pd.DataFrame, y=None):
        self._feature_names_out = []
        required_cols = set()
        for num, den in self.ratio_pairs:
            required_cols.add(num)
            required_cols.add(den)
            self._feature_names_out.append(f"{num}_to_{den}_Ratio")
        missing_cols = required_cols - set(X.columns)
        if missing_cols:
            raise ValueError(f"Missing columns required for ratios: {missing_cols}")
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X_transformed = X.copy()
        logger.debug(f"Calculating ratios: {self.ratio_pairs}")
        for num, den in self.ratio_pairs:
            ratio_col_name = f"{num}_to_{den}_Ratio"
            # Ensure columns are numeric, coerce errors, then fill NaNs
            try:
                num_col = pd.to_numeric(X_transformed[num], errors='coerce')
                den_col = pd.to_numeric(X_transformed[den], errors='coerce')
                X_transformed[ratio_col_name] = num_col / (den_col + self.epsilon)
                # Fill NaNs resulting from division or coercion
                X_transformed[ratio_col_name].fillna(0, inplace=True)
            except KeyError as e:
                 logger.error(f"Column not found during ratio calculation: {e}")
                 raise # Re-raise error as this indicates a fundamental problem
            except Exception as e:
                 logger.error(f"Error calculating ratio {ratio_col_name}: {e}", exc_info=True)
                 # Decide how to handle - fill with 0 or raise? Raising is safer.
                 raise
        return X_transformed

    def get_feature_names_out(self, input_features=None):
        # input_features is the list of column names passed to fit/transform
        if input_features is None:
             # This case is less common with sklearn >= 1.0 but handle defensively
             logger.warning("input_features not provided to get_feature_names_out for FeatureRatioCalculator")
             # Return only the new features if input is unknown, caller must handle merging
             return list(self._feature_names_out)
        # Return original features + new ratio features
        return list(input_features) + self._feature_names_out

class AgeBinner(BaseEstimator, TransformerMixin):
    """Bins the 'Age' column into categories."""
    def __init__(self, bins=[0, 30, 40, 50, 60, 100], labels=['<30', '30-39', '40-49', '50-59', '60+']):
        self.bins = bins
        self.labels = labels
        self.feature_name_out_ = "Age_Bin" # Name of the new feature

    def fit(self, X, y=None):
        if 'Age' not in X.columns:
            raise ValueError("Input DataFrame must contain 'Age' column for AgeBinner.")
        return self

    def transform(self, X):
        X_transformed = X.copy()
        logger.debug("Binning Age feature...")
        try:
            X_transformed[self.feature_name_out_] = pd.cut(
                X_transformed['Age'],
                bins=self.bins,
                labels=self.labels,
                right=False, # [min, max) intervals
                include_lowest=True # Ensure the lowest value is included
            )
            # Convert the new column to object type for consistent handling by OneHotEncoder
            X_transformed[self.feature_name_out_] = X_transformed[self.feature_name_out_].astype(object)
        except KeyError:
             logger.error("Column 'Age' not found during AgeBinner transform.")
             raise
        except Exception as e:
             logger.error(f"Error during Age binning: {e}", exc_info=True)
             raise
        # Decide whether to drop original 'Age'. Let's keep it for now.
        return X_transformed

    def get_feature_names_out(self, input_features=None):
        if input_features is None:
             logger.warning("input_features not provided to get_feature_names_out for AgeBinner")
             return [self.feature_name_out_]
        # Return original features + new binned feature
        return list(input_features) + [self.feature_name_out_]


# --- Data Loading and Processing Functions ---

def load_raw_data(file_path: Path) -> pd.DataFrame:
    """Loads raw data from CSV, handling potential encoding issues."""
    logger.info(f"Loading raw data from {file_path}...")
    if not file_path.exists():
         logger.error(f"Data file not found at {file_path}")
         raise FileNotFoundError(f"Data file not found at {file_path}")
    try:
        # Try standard UTF-8 first
        df = pd.read_csv(file_path, encoding='utf-8')
    except UnicodeDecodeError:
        logger.warning("UTF-8 decoding failed, trying 'latin-1' encoding...")
        try:
            df = pd.read_csv(file_path, encoding='latin-1')
        except Exception as e:
            logger.error(f"Failed to load raw data with UTF-8 and latin-1 encoding from {file_path}: {e}")
            raise
    except pd.errors.EmptyDataError:
         logger.error(f"No data found in file: {file_path}")
         raise
    except Exception as e:
        logger.error(f"Error loading raw data from {file_path}: {e}", exc_info=True)
        raise

    logger.info(f"Raw data loaded successfully. Shape: {df.shape}")
    # Optional: Log column names and dtypes for debugging
    # logger.debug(f"Raw columns: {df.columns.tolist()}")
    # logger.debug(f"Raw dtypes:\n{df.dtypes}")
    return df

def drop_irrelevant_features(df: pd.DataFrame) -> pd.DataFrame:
     """Drops predefined irrelevant columns if they exist."""
     cols_to_drop = ['RowNumber', 'CustomerId', 'Surname']
     cols_to_drop_existing = [col for col in cols_to_drop if col in df.columns]
     if cols_to_drop_existing:
         df = df.drop(columns=cols_to_drop_existing)
         logger.info(f"Dropped irrelevant columns: {cols_to_drop_existing}")
     else:
         logger.debug("No standard irrelevant columns found to drop.")
     return df

def select_initial_features(df: pd.DataFrame, initial_features: List[str], target_column: str) -> pd.DataFrame:
    """Selects only the features specified in config + target, ensuring they exist."""
    logger.debug(f"Selecting initial features: {initial_features}")
    cols_to_keep = initial_features + [target_column]
    # Check which requested columns are actually present
    cols_present = [col for col in cols_to_keep if col in df.columns]
    missing_cols = set(cols_to_keep) - set(cols_present)

    if missing_cols:
        # Log error but proceed with available columns? Or raise error? Raising is safer.
        logger.error(f"Initial features/target missing from DataFrame after cleaning: {missing_cols}")
        raise ValueError(f"Columns specified in config.data.initial_features or target_column not found: {missing_cols}")

    if set(cols_present) != set(cols_to_keep):
        logger.warning(f"Not all requested initial features were found. Using: {cols_present}")

    return df[cols_present]

def split_data(df: pd.DataFrame, config: DataConfig) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """Splits data into training and testing sets with stratification."""
    target_column = config.target_column
    test_size = config.test_size
    random_state = config.random_state

    if target_column not in df.columns:
        logger.error(f"Target column '{target_column}' not found for splitting.")
        raise ValueError(f"Target column '{target_column}' not found.")
    if df[target_column].isnull().any():
         logger.error(f"Target column '{target_column}' contains missing values. Cannot stratify.")
         raise ValueError(f"Target column '{target_column}' contains missing values.")

    logger.info(f"Splitting data with test_size={test_size}, random_state={random_state}...")
    try:
        X = df.drop(target_column, axis=1)
        y = df[target_column]
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=test_size,
            random_state=random_state,
            stratify=y # Stratify based on the target variable
        )
        logger.info(f"Data split complete. Train shape: {X_train.shape}, Test shape: {X_test.shape}")
        return X_train, X_test, y_train, y_test
    except Exception as e:
        logger.error(f"Error during data splitting: {e}", exc_info=True)
        raise

def save_processed_data(X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series, y_test: pd.Series, config: DataConfig):
    """Saves processed data splits to parquet files."""
    train_path = Path(config.processed_train_path)
    test_path = Path(config.processed_test_path)

    try:
        train_path.parent.mkdir(parents=True, exist_ok=True)
        test_path.parent.mkdir(parents=True, exist_ok=True)
    except OSError as e:
         logger.error(f"Could not create directories for processed data: {e}")
         raise

    try:
        # Combine X and y for saving, makes loading simpler
        train_df = X_train.copy()
        train_df[config.target_column] = y_train
        test_df = X_test.copy()
        test_df[config.target_column] = y_test

        logger.info(f"Saving processed training data to {train_path}...")
        train_df.to_parquet(train_path, index=False, engine='pyarrow') # Specify engine
        logger.info(f"Saving processed testing data to {test_path}...")
        test_df.to_parquet(test_path, index=False, engine='pyarrow')
        logger.info("Processed data saved successfully.")
    except ImportError:
        logger.error("`pyarrow` engine not installed. Cannot save to parquet. Please install it (`poetry add pyarrow`).")
        raise
    except Exception as e:
        logger.error(f"Error saving processed data to parquet: {e}", exc_info=True)
        raise

def load_processed_data(config: DataConfig) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """Loads processed data splits from parquet files."""
    train_path = Path(config.processed_train_path)
    test_path = Path(config.processed_test_path)
    target_column = config.target_column

    if not train_path.exists():
        logger.error(f"Processed training data file not found at {train_path}")
        raise FileNotFoundError(f"Processed training data not found at {train_path}. Run data processing first.")
    if not test_path.exists():
        logger.error(f"Processed testing data file not found at {test_path}")
        raise FileNotFoundError(f"Processed testing data not found at {test_path}. Run data processing first.")

    try:
        logger.info(f"Loading processed training data from {train_path}...")
        train_df = pd.read_parquet(train_path, engine='pyarrow')
        logger.info(f"Loading processed testing data from {test_path}...")
        test_df = pd.read_parquet(test_path, engine='pyarrow')

        # Separate features and target
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        X_test = test_df.drop(columns=[target_column])
        y_test = test_df[target_column]

        logger.info("Processed data loaded successfully.")
        return X_train, X_test, y_train, y_test
    except ImportError:
        logger.error("`pyarrow` engine not installed. Cannot load from parquet. Please install it (`poetry add pyarrow`).")
        raise
    except Exception as e:
        logger.error(f"Error loading processed data from parquet: {e}", exc_info=True)
        raise

def run_preprocess_workflow(config: AppConfig) -> bool:
    """Executes the full data loading, cleaning, splitting, and saving workflow."""
    logger.info("--- Starting Data Preprocessing Workflow ---")
    data_cfg = config.data
    success = False
    try:
        raw_df = load_raw_data(Path(data_cfg.raw_path))
        clean_df = drop_irrelevant_features(raw_df)
        selected_df = select_initial_features(clean_df, data_cfg.initial_features, data_cfg.target_column)
        X_train, X_test, y_train, y_test = split_data(selected_df, data_cfg)
        save_processed_data(X_train, X_test, y_train, y_test, data_cfg)
        success = True
        logger.info("--- Data Preprocessing Workflow Completed Successfully ---")
    except FileNotFoundError as e:
        logger.error(f"Data preprocessing failed: {e}")
    except ValueError as e:
         logger.error(f"Data preprocessing failed due to value error: {e}")
    except Exception as e:
        logger.error(f"An unexpected error occurred during data preprocessing: {e}", exc_info=True)

    return success

# --- Optional: Add command-line execution ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run data preprocessing workflow.")
    parser.add_argument(
        "--config", default="config.yaml", help="Path to the configuration file relative to project root."
    )
    # Add flag to explicitly run the workflow
    parser.add_argument(
        "--run", action="store_true", help="Run the full preprocessing workflow."
    )
    args = parser.parse_args()

    # Setup logging when run as script
    setup_logging()

    if args.run:
        try:
            config_path = PROJECT_ROOT / args.config
            # Need to load config using the function from config module
            from .config import load_config
            app_config = load_config(config_path)
            run_preprocess_workflow(app_config)
        except Exception as e:
            logger.critical(f"Preprocessing script failed: {e}", exc_info=True)
            exit(1)
    else:
        logger.info("Processing module loaded. Use --run flag to execute the preprocessing workflow.")
```

```python
# ============================================
# File: src/churn_model/pipeline.py
# ============================================
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.impute import SimpleImputer
import numpy as np
import pandas as pd
from loguru import logger

# Import custom transformers defined in processing.py
from .processing import FeatureRatioCalculator, AgeBinner
# No longer need DataConfig here if lists are passed directly

def create_data_processing_pipeline(numerical_vars: list, categorical_vars: list) -> Pipeline:
    """
    Creates the full data preprocessing and feature engineering pipeline.
    Uses make_column_selector for robustness. Requires sklearn >= 0.24 approx.
    Sets pipeline output to pandas DataFrame.

    Args:
        numerical_vars: List of names of numerical columns *before* FE.
        categorical_vars: List of names of categorical columns *before* FE.

    Returns:
        A Scikit-learn Pipeline object ready for fitting or transforming data.
    """
    logger.info("Creating data processing pipeline...")
    logger.debug(f"Initial numerical vars: {numerical_vars}")
    logger.debug(f"Initial categorical vars: {categorical_vars}")

    # --- Define Feature Engineering Steps ---
    # Ensure these transformers handle DataFrames and implement get_feature_names_out
    ratio_creator = FeatureRatioCalculator(
        ratio_pairs=[('Balance', 'EstimatedSalary'), ('CreditScore', 'Age')]
    )
    age_binner = AgeBinner() # Outputs 'Age_Bin' as object/category type

    # --- Define Preprocessing Steps ---
    numerical_imputer = SimpleImputer(strategy='median')
    scaler = StandardScaler()
    # handle_unknown='ignore' is crucial for predicting on new data
    # sparse_output=False is needed for StandardScaler and many models
    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

    # --- Build the Pipeline ---

    # Step 1: Feature Engineering Pipeline
    # Applies custom transformers sequentially. Output should be DataFrame.
    feature_engineering = Pipeline(steps=[
        ('ratio_features', ratio_creator),
        ('age_binning', age_binner)
        # Add other FE steps here if needed
    ])
    # Attempt to set output to pandas for easier handling of feature names
    try:
        feature_engineering.set_output(transform="pandas")
        logger.debug("Set feature_engineering output to pandas.")
    except AttributeError:
        logger.warning("Could not set feature_engineering output to pandas (requires recent sklearn). Feature names might be less readable.")


    # Step 2: Preprocessing using ColumnTransformer
    # Applied *after* feature engineering. Uses selectors for flexibility.
    preprocessor = ColumnTransformer(
        transformers=[
            # Numerical pipeline: Impute then Scale
            # Selects all numeric columns present *after* the FE step
            ('numerical', Pipeline(steps=[
                ('imputer', numerical_imputer),
                ('scaler', scaler)
            ]), make_column_selector(dtype_include=np.number)),

            # Categorical pipeline: Impute (optional) then OneHotEncode
            # Selects object/category columns *after* FE step (e.g., Geography, Gender, Age_Bin)
            ('categorical', Pipeline(steps=[
                # Optional: Impute categorical NaNs if FE might create them
                # ('imputer', SimpleImputer(strategy='most_frequent')),
                ('onehot', encoder)
            ]), make_column_selector(dtype_include=[object, 'category']))
        ],
        remainder='passthrough', # Keep columns not explicitly selected (e.g., binary flags if not in categorical_vars)
                                 # Change to 'drop' if you *only* want the processed num/cat features. Test carefully.
        verbose_feature_names_out=False # Keep feature names cleaner (e.g., 'Geography_France' instead of 'categorical__Geography_France')
    )
    # Attempt to set output to pandas
    try:
        preprocessor.set_output(transform="pandas")
        logger.debug("Set preprocessor output to pandas.")
    except AttributeError:
        logger.warning("Could not set preprocessor output to pandas (requires recent sklearn).")


    # Combine Feature Engineering and Preprocessing sequentially
    data_processing_pipeline = Pipeline(steps=[
        ('feature_engineering', feature_engineering),
        ('preprocessing', preprocessor)
    ])
    # Attempt to set final output to pandas
    try:
        data_processing_pipeline.set_output(transform="pandas")
        logger.debug("Set data_processing_pipeline output to pandas.")
    except AttributeError:
         logger.warning("Could not set data_processing_pipeline output to pandas (requires recent sklearn). Output might be numpy array.")


    logger.info("Data processing pipeline created successfully.")
    return data_processing_pipeline
```

```python
# ============================================
# File: src/churn_model/tune.py
# ============================================
import optuna
import mlflow
# import mlflow.sklearn # Not explicitly used here, but good practice if logging models directly
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import get_scorer # make_scorer not strictly needed for roc_auc
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from loguru import logger
import numpy as np
import pandas as pd
from pathlib import Path
import json
import argparse
import time # To measure time

# Import project structure and utilities
from .config import load_config, AppConfig
from .processing import load_processed_data, run_preprocess_workflow # Import workflow function
from .pipeline import create_data_processing_pipeline
from .utils import setup_logging, save_json

# --- Setup ---
# Setup logging at the very beginning of the script execution
setup_logging()

# --- Model Definitions ---
def get_base_models(config: AppConfig) -> dict:
    """Returns dictionary of instantiated base models."""
    random_state = config.data.random_state
    # Ensure models are instantiated here
    return {
        "LogisticRegression": LogisticRegression(random_state=random_state, max_iter=2000, class_weight='balanced', solver='saga'), # Use saga for more penalty options
        "RandomForest": RandomForestClassifier(random_state=random_state, class_weight='balanced', n_jobs=-1),
        "LightGBM": LGBMClassifier(random_state=random_state, class_weight='balanced', n_jobs=-1),
        "XGBoost": XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric='logloss', n_jobs=-1), # scale_pos_weight set in objective
        "SVC": SVC(random_state=random_state, probability=True, class_weight='balanced', cache_size=500) # Increase cache size for SVC
    }

# --- Optuna Hyperparameter Spaces ---
def get_optuna_params(trial: optuna.Trial, model_name: str, y_train_for_imbalance: np.ndarray = None):
    """Defines the hyperparameter search space for Optuna."""
    logger.trace(f"Getting Optuna params for trial {trial.number}, model: {model_name}") # Use trace for very verbose logs

    if model_name == "LogisticRegression":
        # Using saga solver allows l1, l2, elasticnet, none
        penalty = trial.suggest_categorical("penalty", ["l1", "l2", "elasticnet", "none"])
        # C is inverse regularization strength; smaller values specify stronger regularization.
        c_val = trial.suggest_float("C", 1e-3, 1e2, log=True)
        params = {"classifier__C": c_val, "classifier__solver": "saga", "classifier__penalty": penalty}
        if penalty == "elasticnet":
            params["classifier__l1_ratio"] = trial.suggest_float("l1_ratio", 0.05, 0.95) # Avoid 0 and 1
        return params

    elif model_name == "RandomForest":
        return {
            "classifier__n_estimators": trial.suggest_int("n_estimators", 50, 500, step=50),
            "classifier__max_depth": trial.suggest_int("max_depth", 5, 30),
            "classifier__min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
            "classifier__min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 20),
            "classifier__max_features": trial.suggest_categorical("max_features", ['sqrt', 'log2', 0.5, 0.7, None]) # Add float options
            }

    elif model_name == "LightGBM":
         return {
            'classifier__n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),
            'classifier__learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),
            'classifier__num_leaves': trial.suggest_int('num_leaves', 10, 150), # Reduced upper bound slightly
            'classifier__max_depth': trial.suggest_int('max_depth', 3, 15),
            'classifier__reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'classifier__reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'classifier__colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
            'classifier__subsample': trial.suggest_float('subsample', 0.5, 1.0),
            # 'classifier__boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']) # Goss might be unstable
         }

    elif model_name == "XGBoost":
         scale_pos_weight = 1.0 # Default
         if y_train_for_imbalance is not None and len(y_train_for_imbalance) > 0:
             try:
                 counts = np.bincount(y_train_for_imbalance)
                 if len(counts) > 1 and counts[1] > 0:
                     scale_pos_weight = float(counts[0]) / counts[1] # Ensure float
                 logger.trace(f"Calculated scale_pos_weight for XGBoost: {scale_pos_weight:.2f}")
             except Exception as e:
                 logger.warning(f"Could not calculate scale_pos_weight: {e}. Using default 1.0.")
                 scale_pos_weight = 1.0

         return {
            'classifier__n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),
            'classifier__learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),
            'classifier__max_depth': trial.suggest_int('max_depth', 3, 15),
            'classifier__subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'classifier__colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
            'classifier__gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),
            'classifier__reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'classifier__reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'classifier__scale_pos_weight': scale_pos_weight # Set dynamically
         }

    elif model_name == "SVC":
         kernel = trial.suggest_categorical("kernel", ["rbf", "poly", "sigmoid"]) # Linear less likely needed
         params = {
             'classifier__C': trial.suggest_float('C', 1e-2, 1e3, log=True), # Wider C range
             'classifier__kernel': kernel,
             # Gamma only relevant for rbf, poly, sigmoid
             'classifier__gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),
         }
         if kernel == "poly":
             params['classifier__degree'] = trial.suggest_int('degree', 2, 4) # Degree 2, 3, 4
         # Coef0 only relevant for poly, sigmoid
         if kernel in ["poly", "sigmoid"]:
              params['classifier__coef0'] = trial.suggest_float('coef0', 0.0, 1.0)
         return params

    else:
        logger.warning(f"Model {model_name} not found in Optuna parameter definitions.")
        return {}


# --- Optuna Objective Function ---
def objective(trial: optuna.Trial, model_name: str, X_train: pd.DataFrame, y_train: pd.Series, config: AppConfig):
    """Optuna objective function for hyperparameter tuning."""
    start_time = time.time()
    base_models = get_base_models(config)
    tuning_cfg = config.tuning
    data_cfg = config.data

    # 1. Create Data Processing Pipeline
    # This is created fresh for each trial, ensuring no state leakage
    data_processing_pipeline = create_data_processing_pipeline(
        numerical_vars=data_cfg.numerical_vars,
        categorical_vars=data_cfg.categorical_vars
    )

    # 2. Get Base Model Instance
    if model_name not in base_models:
        logger.error(f"Base model definition not found for {model_name}")
        raise optuna.exceptions.TrialPruned(f"Model {model_name} not defined.")
    # Create a new instance from the class stored in base_models dict
    base_model = base_models[model_name].__class__(**base_models[model_name].get_params())

    # 3. Create Full Scikit-learn Pipeline
    pipeline = Pipeline(steps=[
        ('data_processing', data_processing_pipeline),
        ('classifier', base_model)
    ])

    # 4. Get & Set Hyperparameters
    params_to_tune = get_optuna_params(trial, model_name, y_train_for_imbalance=y_train.to_numpy())
    # Clean params (e.g., remove l1_ratio if not applicable for LogisticRegression)
    if model_name == "LogisticRegression":
        penalty = params_to_tune.get("classifier__penalty")
        if penalty != "elasticnet":
            params_to_tune.pop("classifier__l1_ratio", None) # Remove if exists and not applicable
    try:
        pipeline.set_params(**params_to_tune)
        logger.trace(f"Trial {trial.number}: Set params {params_to_tune}")
    except ValueError as e:
         logger.warning(f"Error setting parameters for {model_name} trial {trial.number}: {params_to_tune}. Error: {e}")
         raise optuna.exceptions.TrialPruned(f"Incompatible parameters: {e}")

    # 5. Perform Cross-Validation
    cv = StratifiedKFold(n_splits=tuning_cfg.cv_folds, shuffle=True, random_state=data_cfg.random_state)
    optimization_metric = tuning_cfg.optimization_metric
    try:
        # get_scorer is sufficient for standard metrics like 'roc_auc'
        scorer = get_scorer(optimization_metric)
    except ValueError:
         logger.error(f"Invalid optimization_metric '{optimization_metric}' specified.")
         raise optuna.exceptions.TrialPruned(f"Invalid scorer: {optimization_metric}")

    # 6. MLflow Logging (Nested Run)
    # Use the parent run context established in run_tuning
    with mlflow.start_run(nested=True, run_name=f"Trial_{trial.number}") as trial_run:
        # Log Optuna suggested params (remove classifier__ prefix for cleaner MLflow UI)
        mlflow.log_params({k.replace('classifier__', ''): v for k, v in trial.params.items() if v is not None})
        mlflow.log_param("model_name", model_name) # Log model name within trial too
        mlflow.set_tag("optuna_trial_number", trial.number)

        try:
            logger.debug(f"Trial {trial.number}: Starting CV with {tuning_cfg.cv_folds} folds...")
            # CRUCIAL: Use n_jobs=1 for cross_val_score to prevent issues with Optuna/MLflow
            scores = cross_val_score(
                pipeline, X_train, y_train,
                n_jobs=1, # Avoid nested parallelism
                cv=cv,
                scoring=scorer,
                error_score='raise' # Raise errors during CV
            )
            mean_score = float(np.mean(scores)) # Ensure float type
            std_score = float(np.std(scores))
            duration = time.time() - start_time
            logger.debug(f"Trial {trial.number}: CV scores = {scores}, Mean = {mean_score:.5f}, Std = {std_score:.5f}, Duration = {duration:.2f}s")

            mlflow.log_metric(f"cv_{optimization_metric}_mean", mean_score)
            mlflow.log_metric(f"cv_{optimization_metric}_std", std_score)
            mlflow.log_metric("cv_duration_seconds", duration)
            mlflow.set_tag("status", "completed")

        except Exception as e:
             duration = time.time() - start_time
             logger.error(f"CV failed for {model_name} trial {trial.number} after {duration:.2f}s: {e}", exc_info=True)
             mlflow.log_metric(f"cv_{optimization_metric}_mean", -np.inf) # Log failure clearly
             mlflow.log_metric("cv_duration_seconds", duration)
             mlflow.set_tag("status", "failed_cv")
             # Pruning tells Optuna this trial failed badly
             raise optuna.exceptions.TrialPruned(f"Cross-validation failed: {e}")

    # Return the mean score for Optuna to optimize
    return mean_score

# --- Main Tuning Function ---
def run_tuning(config: AppConfig):
    """Orchestrates the hyperparameter tuning process."""
    logger.info("--- Starting Hyperparameter Tuning Workflow ---")
    tuning_cfg = config.tuning
    data_cfg = config.data

    # --- Load Data ---
    # Attempt to load processed data first
    try:
        X_train, X_test, y_train, y_test = load_processed_data(data_cfg)
        logger.info("Loaded processed data for tuning.")
    except FileNotFoundError:
        logger.warning("Processed data not found. Attempting to run preprocessing workflow...")
        # Run the preprocessing workflow defined in processing.py
        if run_preprocess_workflow(config):
            logger.info("Preprocessing workflow completed. Reloading processed data...")
            try:
                X_train, X_test, y_train, y_test = load_processed_data(data_cfg)
            except Exception as e:
                 logger.critical(f"Failed to load processed data even after running workflow: {e}", exc_info=True)
                 raise RuntimeError("Failed to load data for tuning.") from e
        else:
            logger.critical("Preprocessing workflow failed. Cannot proceed with tuning.")
            raise RuntimeError("Data preprocessing failed.")
    except Exception as e:
         logger.critical(f"Failed to load initial processed data: {e}", exc_info=True)
         raise RuntimeError("Failed to load data for tuning.") from e


    # --- MLflow Setup ---
    mlflow.set_experiment(tuning_cfg.mlflow_experiment_name)
    logger.info(f"MLflow Experiment set to: {tuning_cfg.mlflow_experiment_name}")

    overall_best_score = -np.inf
    overall_best_params = None
    overall_best_model_name = None
    all_study_results = {}

    # --- Loop Through Models ---
    for model_name in tuning_cfg.models_to_tune:
        logger.info(f"\n===== Tuning Model: {model_name} =====")
        # Parent run for each model type tuning process
        with mlflow.start_run(run_name=f"Tune_{model_name}") as parent_run:
            parent_run_id = parent_run.info.run_id
            logger.info(f"Started MLflow parent run for {model_name}: {parent_run_id}")
            mlflow.log_param("model_type", model_name)
            mlflow.set_tag("tuning_status", "running")

            study = optuna.create_study(
                direction="maximize",
                study_name=f"{model_name}_tuning_{parent_run_id}", # Link study name to run_id
                pruner=optuna.pruners.MedianPruner(n_warmup_steps=5, n_min_trials=10) # Prune slow/bad trials
            )
            try:
                study.optimize(
                    # Use lambda to pass arguments to objective
                    lambda trial: objective(trial, model_name, X_train, y_train, config),
                    n_trials=tuning_cfg.optuna_trials_per_model,
                    n_jobs=1, # Run trials sequentially for safety with logging/complex pipelines
                    show_progress_bar=True,
                    # Optional: Add timeout per model type
                    # timeout=3600 # e.g., 1 hour timeout
                )
                mlflow.set_tag("tuning_status", "completed")
                logger.info(f"Optuna tuning completed for {model_name}.")
            except Exception as e:
                # Catch errors during optimize call itself (less common)
                logger.error(f"Optuna study optimize call failed for {model_name}: {e}", exc_info=True)
                mlflow.set_tag("tuning_status", "failed")
                continue # Skip to next model if study fails

            # --- Process Study Results ---
            try:
                best_trial = study.best_trial
                current_best_score = best_trial.value

                all_study_results[model_name] = {
                    "best_score": current_best_score,
                    "best_params": best_trial.params, # Keep original names with classifier__
                    "mlflow_run_id": parent_run_id
                }

                logger.info(f"Best Trial Number for {model_name}: {best_trial.number}")
                logger.info(f"Best CV {tuning_cfg.optimization_metric} Score: {current_best_score:.5f}")
                logger.info(f"Best Params: {best_trial.params}")

                # Log best params/score to parent run
                best_params_cleaned = {k.replace('classifier__', ''): v for k, v in best_trial.params.items() if v is not None}
                mlflow.log_params(best_params_cleaned)
                mlflow.log_metric(f"best_cv_{tuning_cfg.optimization_metric}", current_best_score)
                mlflow.set_tag("best_trial_number", best_trial.number)

                # Check if this model is the best overall so far
                # Ensure score is not -np.inf (which indicates failure)
                if current_best_score > -np.inf and current_best_score > overall_best_score:
                    overall_best_score = current_best_score
                    overall_best_params = best_trial.params # Store params with classifier__ prefix
                    overall_best_model_name = model_name
                    logger.success(f"*** New Overall Best Model Found: {model_name} (CV Score: {overall_best_score:.5f}) ***") # Use success level
                    mlflow.set_tag("is_current_best", "True") # Tag the run

            except ValueError:
                 # Handle case where study completed but no trials were successful (e.g., all pruned)
                 logger.warning(f"Optuna study for {model_name} finished without any successful trials.")
                 mlflow.set_tag("tuning_status", "no_successful_trials")
            except Exception as e:
                 logger.error(f"Error processing Optuna results for {model_name}: {e}", exc_info=True)
                 mlflow.set_tag("tuning_status", "result_processing_error")


    # --- Save Best Overall Parameters ---
    if overall_best_model_name and overall_best_params:
        logger.info(f"\n===== Overall Best Model from Tuning =====")
        logger.info(f"Best Model Type: {overall_best_model_name}")
        logger.info(f"Best CV Score: {overall_best_score:.5f}")
        logger.info(f"Best Hyperparameters: {overall_best_params}")

        best_params_data = {
            "best_model_name": overall_best_model_name,
            "best_cv_score": overall_best_score,
            "best_hyperparameters": overall_best_params # Save with classifier__ prefix
        }
        output_path = Path(tuning_cfg.best_params_output_path)
        try:
            save_json(best_params_data, output_path)
            logger.info(f"Best overall parameters saved to: {output_path}")

            # Log summary to a separate MLflow run
            with mlflow.start_run(run_name="Tuning_Summary") as summary_run:
                 mlflow.log_param("overall_best_model", overall_best_model_name)
                 mlflow.log_metric("overall_best_cv_score", overall_best_score)
                 # Log simplified best params
                 mlflow.log_params({f"best_{k.replace('classifier__','')}": v for k,v in overall_best_params.items()})
                 mlflow.log_artifact(str(output_path))
                 logger.info(f"Tuning summary logged to MLflow Run ID: {summary_run.info.run_id}")

        except Exception as e:
             logger.error(f"Failed to save or log best parameters/summary: {e}", exc_info=True)

    else:
        logger.warning("Tuning finished, but no overall best model was identified.")

    logger.info("--- Hyperparameter Tuning Workflow Finished ---")
    # Return path only if successful
    return Path(tuning_cfg.best_params_output_path) if overall_best_model_name else None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run hyperparameter tuning for churn model.")
    parser.add_argument(
        "--config", default="config.yaml", help="Path to the configuration file relative to project root."
    )
    args = parser.parse_args()

    try:
        config_path = PROJECT_ROOT / args.config
        app_config = load_config(config_path)
        run_tuning(app_config)
    except FileNotFoundError:
         logger.critical("Configuration file not found. Please check the path.")
         exit(1)
    except Exception as e:
        logger.critical(f"Tuning script failed: {e}", exc_info=True)
        exit(1)
```

```python
# ============================================
# File: src/churn_model/train.py
# ============================================
import mlflow
import mlflow.sklearn # Import for logging sklearn models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score # For quick eval
from loguru import logger
import numpy as np
import pandas as pd
from pathlib import Path
import json
import argparse
import time

# Import project structure and utilities
from .config import load_config, AppConfig, PROJECT_ROOT # Import PROJECT_ROOT
from .processing import load_processed_data # Or load raw and split here
from .pipeline import create_data_processing_pipeline
from .utils import setup_logging, load_json, save_pipeline_joblib # Removed load_pipeline_joblib as it's not used here

# --- Setup ---
setup_logging()

# --- Model Definitions ---
# Duplicated from tune.py for simplicity, could be refactored into a shared module
def get_base_models(config: AppConfig) -> dict:
    """Returns dictionary of instantiated base models."""
    random_state = config.data.random_state
    return {
        "LogisticRegression": LogisticRegression(random_state=random_state, max_iter=2000, class_weight='balanced', solver='saga'),
        "RandomForest": RandomForestClassifier(random_state=random_state, class_weight='balanced', n_jobs=-1),
        "LightGBM": LGBMClassifier(random_state=random_state, class_weight='balanced', n_jobs=-1),
        "XGBoost": XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric='logloss', n_jobs=-1),
        "SVC": SVC(random_state=random_state, probability=True, class_weight='balanced', cache_size=500)
    }

# --- Main Training Function ---
def run_training(config: AppConfig):
    """Trains the final model using the best hyperparameters found during tuning."""
    logger.info("--- Starting Final Model Training Workflow ---")
    start_time = time.time()
    training_cfg = config.training
    data_cfg = config.data

    # --- Load Best Hyperparameters ---
    best_params_path = Path(training_cfg.best_params_input_path)
    if not best_params_path.exists():
        logger.error(f"Best parameters file not found at {best_params_path}. Run tuning script first.")
        raise FileNotFoundError(f"Best parameters file not found at {best_params_path}")
    try:
        best_params_data = load_json(best_params_path)
        best_model_name = best_params_data.get("best_model_name")
        # These parameters MUST have the 'classifier__' prefix from Optuna study
        best_hyperparameters = best_params_data.get("best_hyperparameters")

        if not best_model_name or not best_hyperparameters:
            logger.error("Best model name or hyperparameters missing from parameters file.")
            raise ValueError("Invalid best parameters file content.")

        logger.info(f"Training final model: {best_model_name}")
        logger.info(f"Using hyperparameters: {best_hyperparameters}")
    except Exception as e:
        logger.error(f"Failed to load or parse best parameters file: {e}", exc_info=True)
        raise

    # --- Load Data ---
    try:
        X_train, X_test, y_train, y_test = load_processed_data(data_cfg)
        logger.info(f"Loaded processed data for final training. Train shape: {X_train.shape}, Test shape: {X_test.shape}")
    except FileNotFoundError:
         logger.error("Processed data not found. Cannot train final model. Run preprocessing/tuning first.")
         raise
    except Exception as e:
         logger.error(f"Failed to load processed data: {e}", exc_info=True)
         raise

    # --- Create Pipeline ---
    base_models = get_base_models(config)
    if best_model_name not in base_models:
        logger.error(f"Best model type '{best_model_name}' not found in defined base models.")
        raise ValueError(f"Unknown best model type: {best_model_name}")

    # Get a fresh instance of the base model
    base_model = base_models[best_model_name].__class__(**base_models[best_model_name].get_params())
    logger.debug(f"Instantiated base model: {base_model}")

    # Create the data processing part of the pipeline
    data_processing_pipeline = create_data_processing_pipeline(
        numerical_vars=data_cfg.numerical_vars,
        categorical_vars=data_cfg.categorical_vars
    )
    # Create the full final pipeline
    final_pipeline = Pipeline(steps=[
        ('data_processing', data_processing_pipeline),
        ('classifier', base_model) # Add the instantiated base model
    ])
    logger.debug("Created final pipeline structure.")


    # --- Set Best Hyperparameters ---
    # Special handling for XGBoost scale_pos_weight if it wasn't tuned/saved
    if best_model_name == "XGBoost" and 'classifier__scale_pos_weight' not in best_hyperparameters:
         try:
             counts = np.bincount(y_train)
             scale_pos_weight = float(counts[0]) / counts[1] if len(counts) > 1 and counts[1] > 0 else 1.0
             best_hyperparameters['classifier__scale_pos_weight'] = scale_pos_weight
             logger.info(f"Dynamically added scale_pos_weight={scale_pos_weight:.2f} for final XGBoost model.")
         except Exception as e:
             logger.warning(f"Could not calculate scale_pos_weight for final XGBoost model: {e}")

    # Clean params if needed (e.g., LogReg l1_ratio) before setting
    if best_model_name == "LogisticRegression":
        penalty = best_hyperparameters.get("classifier__penalty")
        if penalty != "elasticnet":
            best_hyperparameters.pop("classifier__l1_ratio", None)

    try:
        final_pipeline.set_params(**best_hyperparameters)
        logger.info("Successfully set hyperparameters on the final pipeline.")
    except ValueError as e:
         logger.error(f"Error setting final hyperparameters: {best_hyperparameters}. Error: {e}", exc_info=True)
         raise

    # --- MLflow Logging ---
    mlflow.set_experiment(training_cfg.mlflow_experiment_name)
    with mlflow.start_run(run_name=f"Final_Training_{best_model_name}") as run:
        run_id = run.info.run_id
        logger.info(f"MLflow Run ID for final training: {run_id}")
        mlflow.log_param("model_type", best_model_name)
        # Log simplified params
        mlflow.log_params({k.replace('classifier__', ''): v for k, v in best_hyperparameters.items()})
        mlflow.log_artifact(str(best_params_path)) # Log the params file used

        # --- Fit Model ---
        logger.info("Fitting final pipeline on full training data...")
        fit_start_time = time.time()
        try:
            final_pipeline.fit(X_train, y_train)
            fit_duration = time.time() - fit_start_time
            logger.success(f"Final pipeline fitting complete. Duration: {fit_duration:.2f}s") # Use success
            mlflow.log_metric("training_duration_seconds", fit_duration)
            mlflow.set_tag("training_status", "success")
        except Exception as e:
            fit_duration = time.time() - fit_start_time
            logger.error(f"Failed to fit final pipeline after {fit_duration:.2f}s: {e}", exc_info=True)
            mlflow.log_metric("training_duration_seconds", fit_duration)
            mlflow.set_tag("training_status", "failed")
            raise # Stop execution if training fails

        # --- Quick Evaluation on Test Set (Optional but Recommended) ---
        logger.info("Performing quick evaluation on test set...")
        eval_metrics = {}
        try:
            y_pred = final_pipeline.predict(X_test)
            if hasattr(final_pipeline, "predict_proba"):
                y_proba = final_pipeline.predict_proba(X_test)[:, 1]
                eval_metrics["test_roc_auc"] = roc_auc_score(y_test, y_proba)
            else:
                eval_metrics["test_roc_auc"] = np.nan # Indicate missing value

            eval_metrics["test_f1_score"] = f1_score(y_test, y_pred)
            eval_metrics["test_balanced_accuracy"] = balanced_accuracy_score(y_test, y_pred)

            mlflow.log_metrics(eval_metrics)
            logger.info(f"Quick Test ROC AUC: {eval_metrics.get('test_roc_auc', 'N/A'):.4f}")
            logger.info(f"Quick Test F1 Score: {eval_metrics['test_f1_score']:.4f}")
            logger.info(f"Quick Test Balanced Accuracy: {eval_metrics['test_balanced_accuracy']:.4f}")
            mlflow.set_tag("quick_evaluation_status", "success")

        except Exception as e:
            logger.warning(f"Quick evaluation failed: {e}", exc_info=True)
            mlflow.set_tag("quick_evaluation_status", "failed")


        # --- Save Final Model ---
        final_model_path = Path(training_cfg.final_model_output_path)
        logger.info(f"Attempting to save final model to {final_model_path}...")
        try:
            save_pipeline_joblib(final_pipeline, final_model_path)
            logger.success(f"Final trained pipeline saved successfully.") # Use success

            # Log the saved model file as a generic artifact
            mlflow.log_artifact(str(final_model_path), artifact_path="final_model_joblib")

            # Log the model using mlflow.sklearn for better tracking/deployment
            # Infer signature requires MLflow >= 1.14
            try:
                 from mlflow.models.signature import infer_signature
                 # Use processed data for signature if possible, otherwise raw
                 # This might fail if pipeline output isn't DataFrame
                 try:
                      X_train_processed_sample = final_pipeline.named_steps['data_processing'].transform(X_train.head())
                      signature = infer_signature(X_train_processed_sample, final_pipeline.predict(X_train.head()))
                 except Exception:
                      logger.warning("Could not infer signature from processed data, using raw input.")
                      signature = infer_signature(X_train.head()) # Fallback to raw input signature

                 mlflow.sklearn.log_model(
                     sk_model=final_pipeline,
                     artifact_path="final_model_sklearn", # Subdirectory in artifacts
                     signature=signature,
                     input_example=X_train.head(5).to_dict(orient='records') # Log a few examples
                 )
                 logger.info("Final model logged to MLflow using mlflow.sklearn with signature.")
            except ImportError:
                 mlflow.sklearn.log_model(
                     sk_model=final_pipeline,
                     artifact_path="final_model_sklearn"
                 )
                 logger.info("Final model logged to MLflow using mlflow.sklearn (signature requires newer MLflow).")
            except Exception as sig_e:
                 logger.warning(f"Failed to log model with signature: {sig_e}. Logging without signature.")
                 mlflow.sklearn.log_model(
                     sk_model=final_pipeline,
                     artifact_path="final_model_sklearn"
                 )


            mlflow.set_tag("saving_status", "success")

        except Exception as e:
            logger.error(f"Failed to save or log final model: {e}", exc_info=True)
            mlflow.set_tag("saving_status", "failed")
            raise # Stop if saving fails

    total_duration = time.time() - start_time
    logger.info(f"--- Final Model Training Workflow Finished. Total Duration: {total_duration:.2f}s ---")
    return final_model_path

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train the final churn prediction model.")
    parser.add_argument(
        "--config", default="config.yaml", help="Path to the configuration file relative to project root."
    )
    args = parser.parse_args()

    try:
        config_path = PROJECT_ROOT / args.config
        app_config = load_config(config_path)
        run_training(app_config)
    except FileNotFoundError:
         logger.critical("Configuration or parameters file not found. Please check paths and run tuning first.")
         exit(1)
    except Exception as e:
        logger.critical(f"Training script failed: {e}", exc_info=True)
        exit(1)
```

```python
# ============================================
# File: src/churn_model/evaluate.py
# ============================================
import mlflow
import shap
import matplotlib.pyplot as plt
from sklearn.metrics import (
    roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score,
    matthews_corrcoef, confusion_matrix, ConfusionMatrixDisplay, roc_curve, precision_recall_curve
)
from loguru import logger
import numpy as np
import pandas as pd
from pathlib import Path
import argparse
import time

# Import project structure and utilities
from .config import load_config, AppConfig, PROJECT_ROOT # Import PROJECT_ROOT
from .processing import load_processed_data
from .utils import setup_logging, load_pipeline_joblib

# --- Setup ---
setup_logging()

# --- Evaluation Plotting Functions ---

def plot_roc_curve(y_true, y_proba, model_name, output_path: Path):
    """Generates and saves ROC curve plot."""
    try:
        fpr, tpr, _ = roc_curve(y_true, y_proba)
        auc_score = roc_auc_score(y_true, y_proba)
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})', color='darkorange', lw=2)
        plt.plot([0, 1], [0, 1], 'k--', label='Random Guess', color='navy', lw=2, linestyle='--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'{model_name} ROC Curve (Test Set)')
        plt.legend(loc='lower right')
        plt.grid(alpha=0.5)
        plt.savefig(output_path, bbox_inches='tight')
        plt.close()
        logger.info(f"ROC curve saved to {output_path}")
        return True
    except Exception as e:
        logger.error(f"Failed to generate ROC curve: {e}", exc_info=True)
        return False

def plot_confusion_matrix(y_true, y_pred, model_name, output_path: Path):
    """Generates and saves confusion matrix plot."""
    try:
        cm = confusion_matrix(y_true, y_pred)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm) # Default labels are fine for 0, 1
        fig, ax = plt.subplots(figsize=(7, 6))
        disp.plot(cmap=plt.cm.Blues, ax=ax) # Use Blues colormap
        ax.set_title(f'{model_name} Confusion Matrix (Test Set)')
        plt.tight_layout()
        plt.savefig(output_path, bbox_inches='tight')
        plt.close()
        logger.info(f"Confusion matrix saved to {output_path}")
        return True
    except Exception as e:
        logger.error(f"Failed to generate confusion matrix: {e}", exc_info=True)
        return False

def plot_precision_recall_curve(y_true, y_proba, model_name, output_path: Path):
     """Generates and saves Precision-Recall curve plot."""
     try:
         precision, recall, _ = precision_recall_curve(y_true, y_proba)
         # Calculate area under PR curve (Average Precision)
         from sklearn.metrics import average_precision_score
         avg_precision = average_precision_score(y_true, y_proba)
         plt.figure(figsize=(8, 6))
         plt.plot(recall, precision, label=f'{model_name} (AP = {avg_precision:.3f})', color='teal', lw=2)
         plt.xlabel('Recall')
         plt.ylabel('Precision')
         plt.title(f'{model_name} Precision-Recall Curve (Test Set)')
         plt.legend(loc='upper right')
         plt.grid(alpha=0.5)
         plt.savefig(output_path, bbox_inches='tight')
         plt.close()
         logger.info(f"Precision-Recall curve saved to {output_path}")
         return True
     except Exception as e:
         logger.error(f"Failed to generate Precision-Recall curve: {e}", exc_info=True)
         return False

# --- SHAP Analysis Function ---
def run_shap_analysis(pipeline, X_train, X_test, config: AppConfig):
    """Performs SHAP analysis and saves plots."""
    logger.info("--- Starting SHAP Analysis ---")
    shap_start_time = time.time()
    eval_cfg = config.evaluation
    data_cfg = config.data
    output_dir = Path(eval_cfg.shap_plots_output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        # IMPORTANT: Use the fitted preprocessor from the loaded pipeline
        preprocessor_step = pipeline.named_steps['data_processing']
        model_step = pipeline.named_steps['classifier']
        model_type = model_step.__class__.__name__
        logger.info(f"Running SHAP for model type: {model_type}")
    except KeyError as e:
        logger.error(f"Pipeline steps 'data_processing' or 'classifier' not found: {e}. Cannot run SHAP.")
        return False # Indicate failure
    except Exception as e:
        logger.error(f"Error accessing pipeline steps: {e}", exc_info=True)
        return False

    # Preprocess data using the FITTED preprocessor
    logger.info("Preprocessing data for SHAP using fitted preprocessor...")
    try:
        # Ensure pipeline output is pandas for feature names
        X_train_processed = preprocessor_step.transform(X_train)
        X_test_processed = preprocessor_step.transform(X_test)

        # Check if output is DataFrame, try to get columns
        if isinstance(X_train_processed, pd.DataFrame):
            feature_names = X_train_processed.columns.tolist()
            logger.info(f"Data preprocessed for SHAP. Features: {len(feature_names)}")
            # logger.debug(f"Processed feature names: {feature_names}")
        else:
            # Fallback if output is numpy array (less ideal)
            logger.warning("Preprocessor output is not a DataFrame. SHAP plots might lack feature names.")
            # Try to get names from the transformer if possible (might be complex)
            try:
                 feature_names = preprocessor_step.get_feature_names_out()
                 logger.info(f"Attempted to get feature names via get_feature_names_out: {len(feature_names)}")
            except AttributeError:
                 feature_names = [f"feature_{i}" for i in range(X_train_processed.shape[1])]
                 logger.warning("Using generic feature names for SHAP plots.")

    except Exception as e:
        logger.error(f"Failed to preprocess data for SHAP: {e}", exc_info=True)
        return False

    # Initialize SHAP Explainer
    explainer = None
    shap_values = None
    expected_value = None
    # Use full test set for TreeExplainer, subset for KernelExplainer plots
    X_test_subset_for_plot = X_test_processed

    try:
        if model_type in ['RandomForestClassifier', 'LGBMClassifier', 'XGBClassifier']:
            logger.info("Using shap.TreeExplainer.")
            # Provide background data for better calibration, especially for approx feature perturbation
            explainer = shap.TreeExplainer(model_step, data=X_train_processed)
            logger.info("Calculating SHAP values (TreeExplainer)...")
            calc_start = time.time()
            shap_values = explainer.shap_values(X_test_processed) # Explain full test set
            logger.info(f"SHAP values calculated (TreeExplainer) in {time.time() - calc_start:.2f}s")
            expected_value = explainer.expected_value

        elif model_type in ['SVC', 'LogisticRegression']:
            logger.info("Using shap.KernelExplainer (can be slow).")
            # Define the prediction function wrapper - MUST return probabilities
            def predict_proba_wrapper(data_as_np):
                # Convert numpy array back to DataFrame with correct columns
                # Ensure feature_names were successfully obtained
                data_df = pd.DataFrame(data_as_np, columns=feature_names)
                # Use the final model step's predict_proba
                return model_step.predict_proba(data_df)

            num_bg_samples = min(eval_cfg.shap_kernel_background_samples, X_train_processed.shape[0])
            logger.info(f"Creating KernelExplainer background dataset using kmeans ({num_bg_samples} samples)...")
            # Use kmeans for a representative background set
            background_data = shap.kmeans(X_train_processed, num_bg_samples, random_state=data_cfg.random_state)

            explainer = shap.KernelExplainer(predict_proba_wrapper, background_data)

            # Explain a subset for KernelExplainer plots due to speed
            num_test_samples = min(100, X_test_processed.shape[0])
            X_test_subset_for_plot = X_test_processed.iloc[:num_test_samples, :]
            logger.info(f"Calculating SHAP values for {num_test_samples} test samples (KernelExplainer)...")
            calc_start = time.time()
            # nsamples='auto' lets SHAP determine appropriate samples for perturbation
            shap_values = explainer.shap_values(X_test_subset_for_plot, nsamples='auto')
            logger.info(f"SHAP values calculated (KernelExplainer) in {time.time() - calc_start:.2f}s")
            expected_value = explainer.expected_value
        else:
            logger.warning(f"SHAP analysis not implemented for model type: {model_type}.")
            return True # Return True as SHAP wasn't expected to run

        logger.info("SHAP values calculation complete.")

    except Exception as e:
        logger.error(f"Failed during SHAP calculation: {e}", exc_info=True)
        return False # Indicate failure

    # Generate SHAP Plots
    plot_success = True
    if explainer and shap_values is not None:
        logger.info("Generating SHAP plots...")
        try:
            # SHAP values often have shape (n_classes, n_samples, n_features) for classification
            # We usually focus on the positive class (Churn=1) which is index 1
            if isinstance(shap_values, list) and len(shap_values) == 2:
                shap_values_pos = shap_values[1]
                # Expected value might also be a list [exp_class0, exp_class1]
                expected_value_pos = expected_value[1] if isinstance(expected_value, (list, np.ndarray)) and len(expected_value) == 2 else expected_value
                logger.debug("Extracted SHAP values for positive class (1).")
            else:
                # Handle cases where shap_values might be just for the positive class already
                shap_values_pos = shap_values
                expected_value_pos = expected_value
                logger.debug("Using SHAP values directly (assuming positive class or single output).")

            # Ensure data for plots matches the SHAP values calculated (full or subset)
            data_for_plots = X_test_subset_for_plot

            # --- Generate Plots ---
            plot_tasks = {
                "summary_bar": lambda: shap.summary_plot(shap_values_pos, data_for_plots, plot_type="bar", show=False, feature_names=feature_names),
                "summary_dot": lambda: shap.summary_plot(shap_values_pos, data_for_plots, show=False, feature_names=feature_names),
            }

            for plot_name, plot_func in plot_tasks.items():
                 try:
                     plt.figure() # Create a new figure context for each plot
                     plot_func()
                     plt.title(f"SHAP {plot_name.replace('_', ' ').title()} ({model_type})")
                     plt.tight_layout()
                     plt.savefig(output_dir / f"{model_type}_shap_{plot_name}.png", bbox_inches='tight')
                     plt.close()
                     logger.info(f"Saved SHAP {plot_name} plot.")
                 except Exception as e_plt:
                     logger.error(f"Failed to generate SHAP {plot_name} plot: {e_plt}", exc_info=True)
                     plot_success = False

            # Dependence Plots (Top N features)
            try:
                 mean_abs_shap = np.abs(shap_values_pos).mean(axis=0)
                 feature_indices = np.argsort(mean_abs_shap)[::-1]
                 num_dep_plots = min(10, len(feature_names)) # Plot top 10
                 logger.info(f"Generating SHAP dependence plots for top {num_dep_plots} features...")
                 for i in range(num_dep_plots):
                     idx = feature_indices[i]
                     # Ensure feature name exists at this index
                     if idx < len(feature_names):
                         name = feature_names[idx]
                         try:
                             plt.figure()
                             shap.dependence_plot(idx, shap_values_pos, data_for_plots, feature_names=feature_names, interaction_index="auto", show=False)
                             plt.title(f"SHAP Dependence: {name} ({model_type})")
                             plt.tight_layout()
                             plt.savefig(output_dir / f"{model_type}_shap_dependence_{name}.png", bbox_inches='tight')
                             plt.close()
                         except Exception as e_dep:
                             logger.warning(f"Could not generate dependence plot for feature '{name}' (index {idx}): {e_dep}")
                     else:
                          logger.warning(f"Feature index {idx} out of bounds for feature names list (length {len(feature_names)}). Skipping dependence plot.")
            except Exception as e_dep_loop:
                 logger.error(f"Error during dependence plot loop: {e_dep_loop}", exc_info=True)
                 plot_success = False

            # Force Plot (First instance of the subset)
            instance_idx = 0
            logger.info(f"Generating SHAP force plot for instance {instance_idx}...")
            try:
                # Use shap.plots.force for saving with matplotlib=True
                # Need to ensure expected_value_pos is a scalar float
                base_value_float = float(expected_value_pos)
                force_plot = shap.force_plot(
                    base_value=base_value_float,
                    shap_values=shap_values_pos[instance_idx,:],
                    features=data_for_plots.iloc[instance_idx,:],
                    feature_names=feature_names,
                    matplotlib=True, # Use matplotlib backend
                    show=False
                )
                plt.title(f"SHAP Force Plot: Instance {instance_idx} ({model_type})")
                # Use plt.savefig directly on the current figure context
                plt.savefig(output_dir / f"{model_type}_shap_force_plot_instance_{instance_idx}.png", bbox_inches='tight')
                plt.close() # Close the current figure
                logger.info(f"SHAP force plot saved for instance {instance_idx}.")
            except Exception as e_force:
                 logger.error(f"Failed to generate or save SHAP force plot: {e_force}", exc_info=True)
                 plot_success = False

            logger.info(f"SHAP plots saved to {output_dir}")

        except Exception as e_plot_main:
            logger.error(f"An error occurred during SHAP plot generation: {e_plot_main}", exc_info=True)
            plot_success = False
    else:
         logger.warning("SHAP explainer or values not available. Skipping plot generation.")
         # If SHAP wasn't expected, this isn't a failure of the SHAP step itself
         plot_success = True if explainer is None else False


    shap_duration = time.time() - shap_start_time
    logger.info(f"--- SHAP Analysis Finished. Duration: {shap_duration:.2f}s ---")
    return plot_success # Return overall success/failure of SHAP step

# --- Main Evaluation Function ---
def run_evaluation(config: AppConfig):
    """Loads the final model and evaluates it on the test set."""
    logger.info("--- Starting Model Evaluation Workflow ---")
    eval_start_time = time.time()
    eval_cfg = config.evaluation
    data_cfg = config.data
    plots_dir = Path(eval_cfg.plots_output_dir)
    plots_dir.mkdir(parents=True, exist_ok=True) # Ensure plots dir exists

    # --- Load Model and Data ---
    try:
        model_path = Path(eval_cfg.model_input_path)
        pipeline = load_pipeline_joblib(model_path)
        logger.info(f"Loaded final model pipeline from {model_path}")
        # Safely get model name
        model_name = "UnknownModel"
        if 'classifier' in pipeline.named_steps:
             model_name = pipeline.named_steps['classifier'].__class__.__name__
        else:
             logger.warning("Could not find 'classifier' step in pipeline to determine model name.")

        X_train, X_test, y_train, y_test = load_processed_data(data_cfg)
        logger.info(f"Loaded processed data for evaluation. Train shape: {X_train.shape}, Test shape: {X_test.shape}")
    except FileNotFoundError as e:
        logger.critical(f"Model or data file not found: {e}. Cannot evaluate.")
        raise
    except Exception as e:
        logger.critical(f"Error loading model or data: {e}", exc_info=True)
        raise

    # --- Make Predictions ---
    logger.info("Making predictions on the test set...")
    y_pred = None
    y_proba = None
    has_proba = False
    try:
        pred_start = time.time()
        y_pred = pipeline.predict(X_test)
        if hasattr(pipeline, "predict_proba"):
            y_proba = pipeline.predict_proba(X_test)[:, 1] # Probability of class 1
            has_proba = True
        else:
            logger.warning(f"Model {model_name} does not support predict_proba.")
        pred_duration = time.time() - pred_start
        logger.info(f"Predictions completed in {pred_duration:.2f}s")
    except Exception as e:
        logger.error(f"Error during prediction on test set: {e}", exc_info=True)
        raise # Stop evaluation if prediction fails

    # --- Calculate Metrics ---
    logger.info("Calculating evaluation metrics...")
    metrics = {}
    metrics_success = True
    try:
        # Only calculate ROC AUC if probabilities are available
        if has_proba:
            metrics['roc_auc'] = roc_auc_score(y_test, y_proba)
        else:
            metrics['roc_auc'] = np.nan # Use NaN to indicate not applicable/calculable

        metrics['f1_score'] = f1_score(y_test, y_pred)
        metrics['balanced_accuracy'] = balanced_accuracy_score(y_test, y_pred)
        metrics['precision'] = precision_score(y_test, y_pred, zero_division=0) # Handle zero division
        metrics['recall'] = recall_score(y_test, y_pred, zero_division=0)
        metrics['mcc'] = matthews_corrcoef(y_test, y_pred)

        logger.info("--- Test Set Metrics ---")
        for name, value in metrics.items():
            logger.info(f"{name}: {value:.4f}")
        logger.info("------------------------")

    except Exception as e:
        logger.error(f"Error calculating metrics: {e}", exc_info=True)
        metrics_success = False # Mark metrics calculation as failed

    # --- Generate Plots ---
    logger.info("Generating evaluation plots...")
    plots_success = True
    plot_files = []
    try:
        # Plot ROC and PR curves only if probabilities are available
        if has_proba:
            roc_path = plots_dir / f"{model_name}_test_roc_curve.png"
            if plot_roc_curve(y_test, y_proba, model_name, roc_path): plot_files.append(roc_path)
            else: plots_success = False

            pr_path = plots_dir / f"{model_name}_test_pr_curve.png"
            if plot_precision_recall_curve(y_test, y_proba, model_name, pr_path): plot_files.append(pr_path)
            else: plots_success = False

        # Always plot confusion matrix
        cm_path = plots_dir / f"{model_name}_test_confusion_matrix.png"
        if plot_confusion_matrix(y_test, y_pred, model_name, cm_path): plot_files.append(cm_path)
        else: plots_success = False

    except Exception as e:
        logger.error(f"Error generating plots: {e}", exc_info=True)
        plots_success = False

    # --- MLflow Logging ---
    logger.info("Logging evaluation results to MLflow...")
    mlflow_success = True
    try:
        mlflow.set_experiment(eval_cfg.mlflow_experiment_name)
        with mlflow.start_run(run_name=f"Evaluation_{model_name}") as run:
            run_id = run.info.run_id
            logger.info(f"MLflow Run ID for evaluation: {run_id}")
            mlflow.log_param("model_name", model_name)
            mlflow.log_param("evaluated_model_path", str(model_path))
            if metrics_success:
                mlflow.log_metrics(metrics)
            else:
                 mlflow.set_tag("metrics_status", "failed")

            if plots_success and plot_files:
                # Log plots directory or individual files
                mlflow.log_artifacts(str(plots_dir), artifact_path="evaluation_plots")
                logger.info(f"Logged evaluation plots from {plots_dir} to MLflow.")
            else:
                 mlflow.set_tag("plots_status", "failed or incomplete")

            # --- Run SHAP Analysis (within MLflow run) ---
            shap_success = run_shap_analysis(pipeline, X_train, X_test, config)
            mlflow.set_tag("shap_status", "success" if shap_success else "failed")

    except Exception as e:
         logger.error(f"Failed to log results to MLflow: {e}", exc_info=True)
         mlflow_success = False

    eval_duration = time.time() - eval_start_time
    logger.info(f"--- Model Evaluation Workflow Finished. Duration: {eval_duration:.2f}s ---")

    # Optionally, return status flags
    # return metrics_success and plots_success and mlflow_success


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate the final churn prediction model.")
    parser.add_argument(
        "--config", default="config.yaml", help="Path to the configuration file relative to project root."
    )
    args = parser.parse_args()

    try:
        config_path = PROJECT_ROOT / args.config
        app_config = load_config(config_path)
        run_evaluation(app_config)
    except FileNotFoundError:
         logger.critical("Configuration or model file not found. Please check paths.")
         exit(1)
    except Exception as e:
        logger.critical(f"Evaluation script failed: {e}", exc_info=True)
        exit(1)
```

```python
# ============================================
# File: src/churn_model/predict.py
# ============================================
import pandas as pd
from loguru import logger
from typing import Dict, List, Optional, Union, Any
import numpy as np
from pathlib import Path

# Import project structure and utilities
from .config import load_config, AppConfig, PROJECT_ROOT # Import PROJECT_ROOT
from .utils import load_pipeline_joblib, setup_logging # Import setup_logging

# --- Global Cache (Load config once, model path once) ---
# Avoid loading the actual model globally for better multi-process/thread safety
_config_cache: Optional[AppConfig] = None
_model_path_cache: Optional[Path] = None
_model_load_error: Optional[str] = None # Store potential loading error

def _initialize_prediction_service():
    """Loads config and model path. To be called once or lazily."""
    global _config_cache, _model_path_cache, _model_load_error
    if _config_cache is not None: # Already initialized
        return

    setup_logging() # Ensure logging is setup when service initializes
    logger.info("Initializing prediction service...")
    try:
        _config_cache = load_config()
        _model_path_cache = Path(_config_cache.training.final_model_output_path)
        # Check if model file exists on initialization
        if not _model_path_cache.exists():
             _model_load_error = f"Model file not found at path: {_model_path_cache}"
             logger.critical(f"CRITICAL: {_model_load_error}")
        else:
             logger.info(f"Prediction service initialized. Model path: {_model_path_cache}")
             _model_load_error = None # Clear any previous error
    except FileNotFoundError as e:
         _model_load_error = f"Configuration file not found: {e}"
         logger.critical(f"CRITICAL: {_model_load_error}")
         _config_cache = None
         _model_path_cache = None
    except Exception as e:
        _model_load_error = f"Failed to initialize prediction service: {e}"
        logger.critical(f"CRITICAL: {_model_load_error}", exc_info=True)
        _config_cache = None
        _model_path_cache = None

# Call initialization when the module is first imported
_initialize_prediction_service()

# --- Prediction Function ---
def make_prediction(*, input_data: Union[pd.DataFrame, List[Dict[str, Any]]]) -> Dict[str, Optional[List[Any]]]:
    """
    Make predictions using the saved final pipeline. Loads model per request.

    Args:
        input_data: Input data as a Pandas DataFrame or a list of dictionaries.
                    Features should match the raw input expected by the pipeline.

    Returns:
        A dictionary containing 'predictions' and optionally 'probabilities'.
        Includes an 'error' key if prediction fails.
    """
    global _config_cache, _model_path_cache, _model_load_error

    # Check initialization status first
    if _model_load_error:
        logger.error(f"Prediction unavailable due to initialization error: {_model_load_error}")
        return {"predictions": None, "probabilities": None, "error": f"Service initialization failed: {_model_load_error}"}
    if _config_cache is None or _model_path_cache is None:
         # This case should ideally be caught by _model_load_error check, but as a safeguard:
         logger.error("Prediction service not initialized correctly (config or model path missing).")
         return {"predictions": None, "probabilities": None, "error": "Prediction service not ready"}

    # --- Load Pipeline (Per Request) ---
    try:
        # logger.debug(f"Loading pipeline from {_model_path_cache} for request...") # Can be noisy
        pipeline = load_pipeline_joblib(_model_path_cache)
    except FileNotFoundError:
        logger.error(f"Model file not found at {_model_path_cache} during request.")
        # Update global error state if file disappears after init? Optional.
        # _model_load_error = "Model file disappeared after initialization."
        return {"predictions": None, "probabilities": None, "error": "Model file not found"}
    except Exception as e:
        logger.error(f"Failed to load prediction pipeline during request: {e}", exc_info=True)
        return {"predictions": None, "probabilities": None, "error": "Failed to load model"}

    # --- Process Input Data ---
    try:
        if isinstance(input_data, list):
            input_df = pd.DataFrame(input_data)
        elif isinstance(input_data, pd.DataFrame):
            input_df = input_data # Already a DataFrame
        else:
            raise TypeError(f"Unsupported input data type: {type(input_data)}")

        # Validate input columns against config.data.initial_features
        expected_cols = set(_config_cache.data.initial_features)
        missing_cols = expected_cols - set(input_df.columns)
        if missing_cols:
            raise ValueError(f"Missing expected input columns: {missing_cols}")
        # Ensure order matches if pipeline is sensitive (unlikely with ColumnTransformer)
        # input_df = input_df[list(_config_cache.data.initial_features)] # Optional reordering

    except (TypeError, ValueError) as e:
         logger.warning(f"Invalid input data format or content: {e}")
         return {"predictions": None, "probabilities": None, "error": f"Invalid input data: {e}"}
    except Exception as e: # Catch potential DataFrame creation errors
         logger.error(f"Error processing input data: {e}", exc_info=True)
         return {"predictions": None, "probabilities": None, "error": "Error processing input data"}


    # --- Make Prediction ---
    logger.info(f"Making prediction on {len(input_df)} sample(s)...")
    try:
        predictions = pipeline.predict(input_df)
        results: Dict[str, Optional[List[Any]]] = {"predictions": predictions.tolist()} # Convert numpy array

        if hasattr(pipeline, "predict_proba"):
             probabilities = pipeline.predict_proba(input_df)[:, 1] # Probability of positive class (Churn=1)
             results["probabilities"] = probabilities.tolist() # Convert numpy array
        else:
             results["probabilities"] = None # Indicate probabilities are unavailable

        logger.info(f"Prediction completed successfully for {len(input_df)} sample(s).")
        results["error"] = None # Indicate success
        return results

    except Exception as e:
        logger.error(f"Error during pipeline prediction: {e}", exc_info=True)
        return {"predictions": None, "probabilities": None, "error": f"Prediction failed: {e}"}

# --- Example Usage (for testing) ---
if __name__ == '__main__':
    # This part will only run when predict.py is executed directly
    # Initialization should have happened on import.
    if _config_cache and not _model_load_error:
        logger.info("Testing make_prediction function...")
        # Example input matching config.yaml initial_features
        test_input_single = [{
            "CreditScore": 650, "Geography": "France", "Gender": "Male",
            "Age": 35, "Tenure": 5, "Balance": 10000.0, "NumOfProducts": 1,
            "HasCrCard": 1, "IsActiveMember": 0, "EstimatedSalary": 50000.0
        }]
        test_input_bulk = [
            {"CreditScore": 700, "Geography": "Spain", "Gender": "Female", "Age": 42, "Tenure": 2, "Balance": 0.0, "NumOfProducts": 2, "HasCrCard": 1, "IsActiveMember": 1, "EstimatedSalary": 100000.0},
            {"CreditScore": 500, "Geography": "Germany", "Gender": "Male", "Age": 55, "Tenure": 8, "Balance": 120000.0, "NumOfProducts": 1, "HasCrCard": 0, "IsActiveMember": 0, "EstimatedSalary": 80000.0},
            {"CreditScore": 600, "Geography": "France", "Gender": "Female", "Age": 25, "Tenure": 1, "Balance": 5000.0, "NumOfProducts": 2, "HasCrCard": 1, "IsActiveMember": 1, "EstimatedSalary": 150000.0}
        ]
        # Test invalid input
        test_input_invalid = [{"CreditScore": 650, "Geography": "Moon", "Gender": "Other"}] # Missing features, invalid enums

        logger.info("\n--- Testing Single Prediction ---")
        result_single = make_prediction(input_data=test_input_single)
        logger.info(f"Single prediction result: {result_single}")

        logger.info("\n--- Testing Bulk Prediction ---")
        result_bulk = make_prediction(input_data=test_input_bulk)
        logger.info(f"Bulk prediction result: {result_bulk}")

        logger.info("\n--- Testing Invalid Input (Missing Features) ---")
        result_invalid = make_prediction(input_data=test_input_invalid) # Should fail validation
        logger.info(f"Invalid input result: {result_invalid}")

        logger.info("\n--- Testing Invalid Input Type ---")
        result_invalid_type = make_prediction(input_data="not a list or dataframe") # Should fail type check
        logger.info(f"Invalid type result: {result_invalid_type}")

    else:
        logger.error("Cannot run prediction test because the service failed to initialize.")
```

```python
# ============================================
# File: src/api/__init__.py
# ============================================
# Makes src/api a Python package
```

```python
# ============================================
# File: src/api/schemas.py
# ============================================
from pydantic import BaseModel, Field, validator
from typing import Optional, List
from enum import Enum

# --- Enums for Validation ---
# Ensure these match the actual data values expected in requests
class GeographyEnum(str, Enum):
    france = 'France'
    spain = 'Spain'
    germany = 'Germany'

class GenderEnum(str, Enum):
    male = 'Male'
    female = 'Female'

# --- Input Schemas ---
# Define input schema matching raw features expected by the pipeline
# (Corresponds to data.initial_features in config.yaml)
class ChurnPredictionInput(BaseModel):
    CreditScore: int = Field(..., ge=0, description="Customer's credit score")
    Geography: GeographyEnum = Field(..., description="Customer's country")
    Gender: GenderEnum = Field(..., description="Customer's gender")
    Age: int = Field(..., ge=18, le=100, description="Customer's age")
    Tenure: int = Field(..., ge=0, description="Number of years as customer")
    Balance: float = Field(..., ge=0, description="Customer's account balance")
    NumOfProducts: int = Field(..., ge=1, le=4, description="Number of products customer uses (usually 1-4)") # Added upper bound
    HasCrCard: int = Field(..., ge=0, le=1, description="Does the customer have a credit card? (1=Yes, 0=No)")
    IsActiveMember: int = Field(..., ge=0, le=1, description="Is the customer an active member? (1=Yes, 0=No)")
    EstimatedSalary: float = Field(..., ge=0, description="Customer's estimated salary")

    class Config:
        schema_extra = {
            "example": {
                "CreditScore": 608, "Geography": "Spain", "Gender": "Female",
                "Age": 41, "Tenure": 1, "Balance": 83807.86, "NumOfProducts": 1,
                "HasCrCard": 0, "IsActiveMember": 1, "EstimatedSalary": 112542.58
            }
        }
        use_enum_values = True # Use enum values ('France') instead of names ('france') in schema output/docs

class BulkChurnPredictionInput(BaseModel):
    inputs: List[ChurnPredictionInput] = Field(..., min_items=1) # Ensure list is not empty

# --- Output Schemas ---
class ChurnPredictionOutput(BaseModel):
    # Use Optional[] for fields that might not be returned on error
    prediction: Optional[int] = Field(None, description="Churn prediction (1=Churn, 0=No Churn)")
    probability_churn: Optional[float] = Field(None, ge=0, le=1, description="Predicted probability of churning (Class 1)")
    error: Optional[str] = Field(None, description="Error message if prediction failed for this input")

class BulkChurnPredictionOutput(BaseModel):
    # Return a list matching the input order
    results: List[ChurnPredictionOutput]
```

```python
# ============================================
# File: src/api/endpoints/predict.py
# ============================================
from fastapi import APIRouter, HTTPException, Body, status
from loguru import logger
import pandas as pd
from typing import List

# Import the prediction function from the model package
# This assumes the predict module handles its own initialization/model loading
from src.churn_model.predict import make_prediction
# Import API schemas
from src.api.schemas import (
    ChurnPredictionInput,
    ChurnPredictionOutput,
    BulkChurnPredictionInput,
    BulkChurnPredictionOutput
)

router = APIRouter()

@router.post(
    "/predict",
    response_model=ChurnPredictionOutput,
    summary="Predict Churn for a Single Customer",
    tags=["Prediction"],
    description="Accepts data for a single customer and returns the churn prediction and probability."
)
async def post_predict_single(
    input_data: ChurnPredictionInput = Body(..., example={ # Add example directly to Body
        "CreditScore": 608, "Geography": "Spain", "Gender": "Female",
        "Age": 41, "Tenure": 1, "Balance": 83807.86, "NumOfProducts": 1,
        "HasCrCard": 0, "IsActiveMember": 1, "EstimatedSalary": 112542.58
    })
) -> ChurnPredictionOutput:
    """Endpoint to predict churn for a single customer instance."""
    logger.info("Received single prediction request.")
    # The `input_data` is already validated by Pydantic by the time it reaches here

    try:
        # Convert Pydantic model to DataFrame for the prediction function
        input_df = pd.DataFrame([input_data.model_dump()]) # Use model_dump() for Pydantic v2
        results = make_prediction(input_data=input_df)

        # Check for errors returned by the prediction function
        if error_msg := results.get("error"): # Use assignment expression (walrus operator)
            logger.error(f"Prediction function returned error: {error_msg}")
            # Map internal error to appropriate HTTP status code
            if "Model not loaded" in error_msg or "unavailable" in error_msg or "initialization failed" in error_msg:
                status_code = status.HTTP_503_SERVICE_UNAVAILABLE
            elif "Invalid input" in error_msg or "Missing input" in error_msg:
                 status_code = status.HTTP_400_BAD_REQUEST
            else: # General prediction failure
                 status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
            raise HTTPException(status_code=status_code, detail=error_msg)

        # Ensure keys exist before accessing, provide default if missing
        prediction = results.get("predictions", [None])[0]
        probability = results.get("probabilities", [None])[0]

        # Check if prediction itself is None (shouldn't happen if no error, but defensive)
        if prediction is None:
             logger.error("Prediction successful but result prediction is None.")
             raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Prediction resulted in null value.")

        logger.info(f"Single prediction successful: Prediction={prediction}, Probability={probability}")
        return ChurnPredictionOutput(
            prediction=prediction,
            probability_churn=probability,
            error=None # Explicitly set error to None on success
        )

    except HTTPException as http_exc:
        # Re-raise exceptions already mapped to HTTP responses
        raise http_exc
    except Exception as e:
        # Catch any other unexpected errors during request handling/prediction call
        logger.exception(f"Unexpected error during single prediction endpoint: {e}") # Log full traceback
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="An unexpected internal server error occurred.")


@router.post(
    "/predict/bulk",
    response_model=BulkChurnPredictionOutput,
    summary="Predict Churn for Multiple Customers",
    tags=["Prediction"],
    description="Accepts a list of customer data objects and returns predictions and probabilities for each."
)
async def post_predict_bulk(
    input_data: BulkChurnPredictionInput = Body(...)
) -> BulkChurnPredictionOutput:
    """Endpoint to predict churn for a list of customer instances."""
    # Input list emptiness check is handled by Pydantic's min_items=1 in schema

    logger.info(f"Received bulk prediction request for {len(input_data.inputs)} inputs.")

    try:
        # Convert list of Pydantic models to DataFrame
        input_df = pd.DataFrame([item.model_dump() for item in input_data.inputs])

        # Call the prediction logic for the whole batch
        results = make_prediction(input_data=input_df)

        # Check for batch-level errors from the prediction function
        if error_msg := results.get("error"):
            logger.error(f"Bulk prediction function returned error: {error_msg}")
            if "Model not loaded" in error_msg or "unavailable" in error_msg or "initialization failed" in error_msg:
                status_code = status.HTTP_503_SERVICE_UNAVAILABLE
            elif "Invalid input" in error_msg or "Missing input" in error_msg:
                 status_code = status.HTTP_400_BAD_REQUEST # Error likely applies to all inputs
            else:
                 status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
            # For bulk, we might choose to return partial results if possible,
            # but raising an error for the whole batch is simpler here.
            raise HTTPException(status_code=status_code, detail=f"Bulk prediction failed: {error_msg}")

        # Process successful results
        output_results = []
        predictions = results.get("predictions", [])
        probabilities = results.get("probabilities") # Might be None or a list

        # Ensure results match input length
        if len(predictions) != len(input_data.inputs):
             logger.error(f"Mismatch between input count ({len(input_data.inputs)}) and prediction count ({len(predictions)}).")
             raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Internal error: Prediction count mismatch.")

        for i in range(len(predictions)):
            pred = predictions[i]
            prob = probabilities[i] if probabilities and i < len(probabilities) else None
            output_results.append(
                ChurnPredictionOutput(prediction=pred, probability_churn=prob, error=None)
            )

        logger.info(f"Bulk prediction successful for {len(output_results)} inputs.")
        return BulkChurnPredictionOutput(results=output_results)

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.exception(f"Unexpected error during bulk prediction endpoint: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="An unexpected internal server error occurred during bulk prediction.")

```

```python
# ============================================
# File: src/api/main.py
# ============================================
from fastapi import FastAPI, APIRouter, Request, status, HTTPException
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError
from loguru import logger
from pathlib import Path
import time # For timing middleware

# Import utility and config loading from churn_model package
# Ensure these imports happen *after* potential PYTHONPATH setup if running differently
try:
    from src.churn_model.utils import setup_logging
    from src.churn_model.config import load_config, AppConfig
    # Import prediction endpoint router
    from src.api.endpoints import predict as predict_endpoint
    # Import the prediction module to check its state
    import src.churn_model.predict as prediction_service
except ImportError as e:
     # Use basic print if imports fail early (logging might not be set up)
     print(f"ERROR: Failed to import necessary modules: {e}. Check PYTHONPATH and project structure.", file=sys.stderr)
     # Exit if core components can't be imported
     import sys
     sys.exit(1)


# --- Load Configuration ---
# Load config early to get API settings, handle failure gracefully
try:
    config = load_config()
    api_cfg = config.api
except Exception as e:
    print(f"ERROR: Failed to load configuration for API: {e}. Using defaults.", file=sys.stderr)
    # Define minimal default config for API startup
    api_cfg = type('obj', (object,), {'title': 'Churn Prediction API (Default)', 'version': '0.0.1'})()


# --- Setup Logging ---
# Call logging setup after potential config load, before creating app
setup_logging()
logger.info("Logging configured for API.")


# --- Create FastAPI App ---
# Use loaded config for title/version
app = FastAPI(
    title=api_cfg.title,
    version=api_cfg.version,
    description="API to predict customer churn based on input features.",
    docs_url="/docs",
    redoc_url="/redoc"
    # Add lifespan context manager later if needed for more complex startup/shutdown
)

# --- Middleware (Example: Request Timing) ---
@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    logger.info(f"{request.method} {request.url.path} - Completed in {process_time:.4f}s - Status: {response.status_code}")
    return response

# --- Exception Handlers ---
# These handlers provide consistent JSON error responses

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    # Pydantic validation errors
    logger.warning(f"Request validation error: {exc.errors()} for request URL: {request.url}")
    # Provide more structured error detail if needed
    # error_details = [{"loc": err["loc"], "msg": err["msg"], "type": err["type"]} for err in exc.errors()]
    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        # content={"detail": error_details},
        content={"detail": exc.errors()}, # Default FastAPI detail is usually good
    )

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
     # Handles errors explicitly raised with HTTPException in endpoints
     logger.error(f"HTTP Exception caught: {exc.status_code} - {exc.detail} for request URL: {request.url}")
     return JSONResponse(
         status_code=exc.status_code,
         content={"detail": exc.detail},
     )

@app.exception_handler(Exception)
async def generic_exception_handler(request: Request, exc: Exception):
     # Catch-all for any other unexpected errors in the application
     logger.exception(f"Unhandled exception during request to {request.url}") # Logs traceback
     return JSONResponse(
         status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
         content={"detail": "An unexpected internal server error occurred."},
     )

# --- Routers ---
# Root endpoint for basic health check
@app.get("/", tags=["Health"], summary="Basic API Health Check")
async def health_check():
    """Returns the operational status of the API and the prediction model."""
    logger.debug("Health check endpoint called.")
    # Check the global error state from the prediction service initialization
    model_status = "error" if prediction_service._model_load_error else "loaded" if prediction_service._config_cache else "not initialized"
    if model_status == "error":
         logger.warning(f"Health check reports model error: {prediction_service._model_load_error}")

    return {"status": "ok", "model_status": model_status}

# Include prediction router with a versioned prefix
api_router_v1 = APIRouter(prefix="/api/v1")
api_router_v1.include_router(predict_endpoint.router, tags=["Prediction"]) # Add tags here too

app.include_router(api_router_v1)

# --- Startup/Shutdown Events ---
# Use lifespan context manager for more robust startup/shutdown (FastAPI >= 0.90)
# from contextlib import asynccontextmanager
# @asynccontextmanager
# async def lifespan(app: FastAPI):
#     # Code to run on startup
#     logger.info("--- Starting Churn Prediction API ---")
#     # Trigger initialization explicitly if not done on import
#     prediction_service._initialize_prediction_service()
#     if prediction_service._model_load_error:
#          logger.critical(f"Model/Config failed to load during startup: {prediction_service._model_load_error}")
#     else:
#          logger.info("Prediction service initialization check complete.")
#     yield
#     # Code to run on shutdown
#     logger.info("--- Shutting Down Churn Prediction API ---")
# app = FastAPI(..., lifespan=lifespan) # Assign lifespan to app

# --- Simpler Startup/Shutdown Events (for older FastAPI or basic needs) ---
@app.on_event("startup")
async def startup_event():
    logger.info("--- API Startup Event Triggered ---")
    # Initialization is handled when predict.py is imported,
    # but we log the status here.
    if prediction_service._model_load_error:
         logger.critical(f"API Startup: Prediction service failed to initialize: {prediction_service._model_load_error}")
    elif prediction_service._config_cache:
         logger.info("API Startup: Prediction service appears initialized.")
    else:
         logger.warning("API Startup: Prediction service initialization status unknown or pending.")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("--- API Shutdown Event Triggered ---")


# --- Run Command Reminder ---
# To run from project root:
# poetry run uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000
# For production, use more workers:
# poetry run uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --workers 4
```
